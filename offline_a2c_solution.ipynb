{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter # type: ignore\n",
    "\n",
    "from rl_envs.gym_grid_world_env import GridWorldEnv\n",
    "from agents.offline_A2C import A2CAgent\n",
    "from tools.helper import *\n",
    "import torch.utils.data\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from torch.nn import functional as F\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 1e-5\n",
    "DISCOUNTED_FACTOR = 0.99\n",
    "\n",
    "FORBIDDEN_REWARD = -10\n",
    "HITWALL_REWARD = -10\n",
    "TARGET_REWARD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# # env = GridWorldEnv(size=3,fixed_map = True, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# model = A2C(\"MultiInputPolicy\", env, tensorboard_log=\"./runs/\", verbose=1)\n",
    "# model.learn(total_timesteps=100_0000) # 所以训练无 fobidden 的地图需要 10_0000 次 (反正 1_0000 是不够的) \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vec_env = GridWorldEnv(size=3,fixed_map = True, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# vec_env = GridWorldEnv( fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD, render_mode=\"human\")\n",
    "\n",
    "# obs, _ = vec_env.reset()\n",
    "# total_reward = 0\n",
    "# for i in range(500):\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, terminated, truncated, info  = vec_env.step(action)\n",
    "#     # vec_env.render()\n",
    "#     # if reward > 0:\n",
    "#         # break\n",
    "#     # VecEnv resets automatically\n",
    "#     total_reward += reward\n",
    "#     if terminated or truncated:\n",
    "#         obs, _ = vec_env.reset()\n",
    "#         print('reward: {}, distance: {}'.format(total_reward, info))\n",
    "#         total_reward = 0\n",
    "#         if truncated:\n",
    "#             print(\"TRUNCATE\")\n",
    "#         else:\n",
    "#             print(\"TERMINATE\")\n",
    "# vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_env = GridWorldEnv( fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD, render_mode=\"human\")\n",
    "# test_obs = {'agent': np.array([0, 0]), 'target': np.array([[3, 2]]), 'forbidden': np.array([[1, 1],\n",
    "#     [1, 2],\n",
    "#     [2, 2],\n",
    "#     [3, 1],\n",
    "#     [3, 3],\n",
    "#     [4, 1]])}\n",
    "\n",
    "# V = {}\n",
    "                                   \n",
    "# observation, _ = model.policy.obs_to_tensor(test_obs)\n",
    "# # obs_as_tensor(agent._last_obs, agent.device)\n",
    "# actions, values, log_prob = model.policy(observation)\n",
    "# for y in range(env.size):\n",
    "#     print(\"[\", end=\" \")\n",
    "#     for x in range(env.size):\n",
    "#         test_obs['agent'] = np.array([y,x])\n",
    "#         observation, _ = model.policy.obs_to_tensor(test_obs)\n",
    "#         actions, state_values, log_prob = model.policy(observation)\n",
    "#         action = np.argmax(actions)\n",
    "#         V[(y,x)] = state_values.item()\n",
    "#         print(test_env.action_mappings[action], end=\" \")\n",
    "#     print(\"]\")\n",
    "\n",
    "# print_by_dict(test_env, V)\n",
    "\n",
    "#        # model.policy.forward(test_obs, deterministic=True)\n",
    "#        # q_values = model.q_net(observation)\n",
    "#        # Q[(y,x)] = q_values\n",
    "\n",
    "# ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = GridWorldEnv(size=3,fixed_map = True, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "env = gym.make('CliffWalking-v0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.autograd.set_detect_anomaly(False, check_nan=True)\n",
    "\n",
    "# agent.RUN(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "collect experience replay\n",
    "\"\"\"\n",
    "episode_len = 1000\n",
    "batch_size = 100\n",
    "\n",
    "obs, _ = env.reset()\n",
    "next_state = torch.tensor(obs, dtype = torch.float)\n",
    "episode = []\n",
    "for _ in range(episode_len):\n",
    "    # generate (state, action, reward, next_state, next_action)\n",
    "    state = next_state\n",
    "    action = env.action_space.sample()\n",
    "    # action_probs = agent.policy_net(state)\n",
    "    # action = torch.max(action_probs, dim=-1).indices\n",
    "    # choose action based on policy\n",
    "    # m = torch.distributions.Categorical(action_probs)\n",
    "    # action = m.sample()\n",
    "    obs, reward, terminated , truncated, info = env.step(action)\n",
    "    next_state = torch.tensor(obs, dtype = torch.float)\n",
    "    episode.append(\n",
    "        {\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "            \"next_state\": next_state,\n",
    "            # \"action_probs\": action_probs\n",
    "        }\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "generate pytorch data iter\n",
    "\"\"\"\n",
    "\n",
    "rewards = []\n",
    "states = []\n",
    "actions = []\n",
    "# action_probss = []\n",
    "next_states = []\n",
    "for i in range(len(episode)):\n",
    "    rewards.append(episode[i][\"reward\"])\n",
    "    actions.append(episode[i][\"action\"])\n",
    "    # action_probss.append(episode[i][\"action_probs\"])\n",
    "    states.append(episode[i][\"state\"])\n",
    "    next_states.append(episode[i][\"next_state\"])\n",
    "reward = torch.tensor(rewards, dtype=torch.float).reshape(-1, 1)\n",
    "next_state = torch.stack(next_states)\n",
    "action = torch.tensor(actions, dtype=torch.int).unsqueeze(1)\n",
    "state = torch.stack(states)\n",
    "# action_probs = torch.stack(action_probss)\n",
    "data_arrays = (state, action, reward, next_state)\n",
    "dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
    "data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x100 and 1x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\offline_a2c_solution.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/offline_a2c_solution.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/offline_a2c_solution.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m state, action, reward, next_state \u001b[39min\u001b[39;00m data_iter:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/offline_a2c_solution.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/offline_a2c_solution.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# calculate advatage (TD-error)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/offline_a2c_solution.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     v_values \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mv_net(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/offline_a2c_solution.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     TD_target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m agent\u001b[39m.\u001b[39mdiscounted_factor \u001b[39m*\u001b[39m agent\u001b[39m.\u001b[39mv_net(next_state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/offline_a2c_solution.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     TD_error \u001b[39m=\u001b[39m TD_target \u001b[39m-\u001b[39m v_values\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x100 and 1x64)"
     ]
    }
   ],
   "source": [
    "agent = A2CAgent(1, int(env.action_space.n), lr_policy=1e-5, lr_v=1e-5, discounted_factor=DISCOUNTED_FACTOR)\n",
    "\"\"\"\n",
    "execute algorithm        \n",
    "\"\"\"\n",
    "\n",
    "writer = SummaryWriter()\n",
    "epochs = 600\n",
    "iter_counter = torch.tensor(0.)\n",
    "for episode in range(epochs):\n",
    "        # next_action = torch.max(self.policy_net(next_state), dim=-1).indices\n",
    "    total_reward = 0\n",
    "    for state, action, reward, next_state in data_iter:\n",
    "\n",
    "        # calculate advatage (TD-error)\n",
    "        v_values = agent.v_net(state)\n",
    "        TD_target = reward + agent.discounted_factor * agent.v_net(next_state)\n",
    "        TD_error = TD_target - v_values\n",
    "\n",
    "        TD_error = (TD_error - TD_error.mean()) / (TD_error.std() + 1e-8) # normalization\n",
    "        # calculate probability of choosing this action if using current policy\n",
    "        action_probs = agent.policy_net(state)\n",
    "        # choose action based on policy\n",
    "        m = torch.distributions.Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "\n",
    "\n",
    "        # Actor: policy update\n",
    "        action_prob = action_probs[torch.arange(len(action_probs)), action.squeeze()].unsqueeze(1)\n",
    "\n",
    "        loss_actor = - TD_error * action_prob * agent.action_space # loss 或有问题\n",
    "        # loss_actor = TD_error * - torch.log(action_prob) * (action_prob * agent.action_space) # loss 或有问题\n",
    "        agent.optimizer_p.zero_grad()\n",
    "        loss_actor.sum().backward(retain_graph=True) # 这里这样是否会出问题\n",
    "        # loss_actor.sum().backward(inputs=list(agent.policy_net.parameters())) # 这里这样是否会出问题\n",
    "\n",
    "        # torch.nn.utils.clip_grad.clip_grad_norm_(self.policy_net.parameters(), 100)\n",
    "        # Critic: value update\n",
    "        agent.optimizer_v.zero_grad()\n",
    "        loss_critic = F.mse_loss(TD_target, v_values) * (action_prob * agent.action_space)\n",
    "        # loss_critic = TD_error * - v_values * (action_prob * agent.action_space)\n",
    "        loss_critic.sum().backward()\n",
    "        # loss_critic.sum().backward(inputs=list(agent.v_net.parameters()))\n",
    "        # torch.nn.utils.clip_grad.clip_grad_norm_(self.v_net.parameters(), 100)\n",
    "        agent.optimizer_p.step()\n",
    "        agent.optimizer_v.step()\n",
    "\n",
    "        writer.add_scalar('TD_error', TD_error.sum(), iter_counter)\n",
    "        writer.add_scalar('loss_critic', loss_critic.sum(), iter_counter)\n",
    "        writer.add_scalar('loss_actor', loss_actor.sum(), iter_counter)\n",
    "        total_reward += reward\n",
    "        iter_counter += 1\n",
    "    # writer.add_scalar('reward', total_reward.sum(), episode)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [100, 4]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead.\n",
    "\n",
    "Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [[0.24770188 0.25310567 0.25212058 0.24707192]] [[0.2529317  0.24682836 0.24636596 0.25387388]] [[0.2536649  0.25368395 0.2462884  0.24636272]] ]\n",
      "[ [[0.24813594 0.25405413 0.24593219 0.25187778]] [[0.2507693  0.25126886 0.2462647  0.25169712]] [[0.24397117 0.2581954  0.24531654 0.2525169 ]] ]\n",
      "[ [[0.24693333 0.25235036 0.25072017 0.2499962 ]] [[0.24877876 0.25035146 0.24969731 0.25117245]] [[0.2522654  0.2532033  0.24497296 0.2495583 ]] ]\n",
      "[ 0.00 0.00 0.00 ]\n",
      "[ 0.00 -10.00 0.00 ]\n",
      "[ 0.00 0.00 1.00 ]\n",
      "\n",
      "[  →   ←   →  ]\n",
      "[  →   ←   →  ]\n",
      "[  →   ←   →  ]\n",
      "[ [[-27.86056]] [[-27.675497]] [[-27.999039]] ]\n",
      "[ [[-27.980997]] [[-22.987865]] [[-28.114763]] ]\n",
      "[ [[-28.340826]] [[-28.401997]] [[-28.892595]] ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# policy = agent.generate_policy_table(env.height, env.width)\n",
    "# print_by_dict(env, policy)\n",
    "# print(env)\n",
    "# for i in range(env.height):\n",
    "#     print(\"[\", end=\" \")\n",
    "#     for j in range(env.width):\n",
    "#         state = (i,j)\n",
    "#         action = np.argmax(policy[state])\n",
    "#         print(env.action_mappings[action], end=\" \")\n",
    "#     print(\"]\")\n",
    "\n",
    "# V = agent.generate_v_table(env.height, env.width)\n",
    "# print_by_dict(env, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -1010, distance: [array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2]), array([2, 2])]\n",
      "TRUNCATE\n",
      "reward: 0, distance: [array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1])]\n",
      "TRUNCATE\n",
      "reward: 0, distance: [array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1]), array([0, 0]), array([0, 1])]\n",
      "TRUNCATE\n",
      "reward: -1010, distance: [array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2]), array([0, 2])]\n",
      "TRUNCATE\n"
     ]
    }
   ],
   "source": [
    "# env = GridWorldEnv(render_mode=\"human\", size=3,fixed_map = True, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "visualize_in_gym(agent, inp_env=\"CliffWalking-v0\")\n",
    "# gridworld_demo(agent, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
