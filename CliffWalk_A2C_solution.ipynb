{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload \n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "%autoreload 2\n",
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter # type: ignore\n",
    "\n",
    "from agents.A2C_cliff import A2CAgent\n",
    "from tools.helper import *\n",
    "import  gymnasium  as gym\n",
    "from rl_envs.new_gym_grid_world_env import GridWorldEnv\n",
    "from torch.nn import functional as F\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from stable_baselines3 import A2C\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CliffWalking-v0\")\n",
    "# # env = GridWorldEnv(size=3,fixed_map = True, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# model = A2C(\"MlpPolicy\", env, tensorboard_log=\"./runs/\", verbose=1)\n",
    "# # model = A2C(\"MultiInputPolicy\", env, tensorboard_log=\"./runs/\", verbose=1)\n",
    "# model.learn(total_timesteps=10_0000) # 所以训练无 fobidden 的地图需要 10_0000 次 (反正 1_0000 是不够的) \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
    "# observation, _  = demo_env.reset()\n",
    "# for _ in range(1000):\n",
    "#     # observation = (observation,)\n",
    "#     action = model.predict(observation)  # agent policy that uses the observation and info\n",
    "#     # insert an algorithm that can interact with env and output an action here\n",
    "#     observation, reward, terminated, truncated, info = demo_env.step(int(action[0]))\n",
    "#     if terminated or truncated:\n",
    "#         observation, info = demo_env.reset()\n",
    "#         # print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 1e-2\n",
    "DISCOUNTED_FACTOR = 0.99\n",
    "\n",
    "FORBIDDEN_REWARD = -10\n",
    "HITWALL_REWARD = -10\n",
    "TARGET_REWARD = 1\n",
    "\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21bf0a472b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = GridWorldEnv(size=3,fixed_map = True, seed=SEED, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "\n",
    "# env = gym.make(\"CliffWalking-v0\")\n",
    "# torch.autograd.set_detect_anomaly(False, check_nan=True)\n",
    "# env.seed(args.seed)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_lengths = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Cliff Walking\n",
    "\n",
    "    0: Move up\n",
    "\n",
    "    1: Move right\n",
    "\n",
    "    2: Move down\n",
    "\n",
    "    3: Move left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dzlu2\\AppData\\Local\\Temp\\ipykernel_25560\\1759458303.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  TD_target = torch.tensor(discounted_reward).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 @ Episode 1/60000 (-1.0, 0.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\CliffWalk_A2C_solution.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_A2C_solution.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (terminated \u001b[39mor\u001b[39;00m truncated):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_A2C_solution.ipynb#X10sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_A2C_solution.ipynb#X10sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(state) \u001b[39m# action 这里也有随机性\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_A2C_solution.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     next_state, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_A2C_solution.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     next_state \u001b[39m=\u001b[39m next_state \u001b[39mif\u001b[39;00m reward \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m \u001b[39melse\u001b[39;00m CLIFF_STATE\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\agents\\A2C_cliff.py:75\u001b[0m, in \u001b[0;36mA2CAgent.get_action\u001b[1;34m(self, in_state, optimal)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action\u001b[39m(\u001b[39mself\u001b[39m, in_state, optimal\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     71\u001b[0m     \u001b[39m# with torch.no_grad(): # 哪里都 no_grad 只会害了你 \u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[39m# state = torch.tensor(in_state, dtype=torch.int64)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[39m# state = torch.nn.functional.one_hot(state, 48)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     action_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net(in_state)\n\u001b[0;32m     76\u001b[0m     \u001b[39m# action_probs = (actions_val/actions_val.sum()).detach().numpy()\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[39mif\u001b[39;00m optimal:\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\agents\\A2C_cliff.py:17\u001b[0m, in \u001b[0;36mPolicyNet.forward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inp):\n\u001b[0;32m     16\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(inp, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[1;32m---> 17\u001b[0m     out1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mone_hot(out, \u001b[39m48\u001b[39;49m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m     out3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(out1)\n\u001b[0;32m     19\u001b[0m     probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(out3, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "agent = A2CAgent(int(env.observation_space.n), int(env.action_space.n),  discounted_factor=0.99999)\n",
    "CLIFF_STATE = 37\n",
    "\n",
    "# env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# agent = PGAgent(2, 5, lr = LEARN_RATE, discounted_factor=DISCOUNTED_FACTOR)\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# agent = PGAgent(4, 2, lr = LEARN_RATE, discounted_factor=DISCOUNTED_FACTOR)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "num_episodes = 60000\n",
    "episode_len = 600\n",
    "trajectory = []\n",
    "obs, _ = env.reset()\n",
    "running_reward = -10\n",
    "episode_rewards = defaultdict(float)\n",
    "TD_step = 1 # perform TD(5)\n",
    "terminated = truncated = False\n",
    "for i_episode  in range(num_episodes):\n",
    "    # 首先, 根据 policy 生成 episode\n",
    "    next_state, _ = env.reset()\n",
    "    episode_record = []\n",
    "    # del agent.saved_log_probs[:]\n",
    "\n",
    "    for _ in range(TD_step-1):\n",
    "        # del agent.saved_log_prob\n",
    "        state = next_state\n",
    "        action = agent.get_action(state) # action 这里也有随机性\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = next_state if reward != -100 else CLIFF_STATE \n",
    "        episode_record.append((state, action, reward, next_state))\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    for t in range(episode_len):\n",
    "        if not (terminated or truncated):\n",
    "            state = next_state\n",
    "            action = agent.get_action(state) # action 这里也有随机性\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = next_state if reward != -100 else CLIFF_STATE\n",
    "            episode_record.append((state, action, reward, next_state))\n",
    "\n",
    "        if len(episode_record) == 0:\n",
    "            break\n",
    "\n",
    "        if not (terminated or truncated):\n",
    "            # with torch.no_grad():\n",
    "            discounted_reward = agent.value_net(next_state) \n",
    "        else: # next state is the end, although maybe I should still use v estimate\n",
    "            discounted_reward = 0\n",
    "        for i, (state, action, reward, next_state) in enumerate(episode_record[::-1]):\n",
    "            discounted_reward = reward + agent.discounted_factor * discounted_reward\n",
    "\n",
    "        # discounted_reward = sum(agent.discounted_factor**i * t[2] for i, t in enumerate(episode_record[t:]))\n",
    "        state, action, reward, next_state = episode_record.pop(0)\n",
    "\n",
    "        # total_return = sum(agent.discounted_factor**i * t.reward for i, t in enumerate(episode_record[t:]))\n",
    "\n",
    "        # value_next = agent.value_net(next_state)\n",
    "        # with torch.no_grad():\n",
    "        v_value = agent.value_net(state).squeeze() \n",
    "\n",
    "        # TD_target = reward + agent.discounted_factor * value_next\n",
    "        TD_target = torch.tensor(discounted_reward).squeeze() \n",
    "        TD_error = TD_target - v_value # TD_error is the advantage (每一处加减, 都要注意 broadcasting)\n",
    "\n",
    "        # Update statistics\n",
    "        episode_rewards[i_episode] += reward\n",
    "        # with torch.no_grad():\n",
    "        action_probs = agent.policy_net(state)\n",
    "        action_prob = action_probs[0][action]\n",
    "\n",
    "        agent.optimizer_v.zero_grad()\n",
    "        loss2 =  F.mse_loss(TD_target, v_value)\n",
    "        loss2.sum().backward(retain_graph=True)\n",
    "        agent.optimizer_v.step()\n",
    "\n",
    "        agent.optimizer.zero_grad()\n",
    "        # loss = -agent.saved_log_prob * TD_error\n",
    "        loss = -action_prob * TD_error\n",
    "        loss.sum().backward()\n",
    "        # torch.nn.utils.clip_grad.clip_grad_norm_(agent.policy_net.parameters(), 100)\n",
    "        agent.optimizer.step()\n",
    "        if t % 1000 == 0:\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({}, {})\".format(\n",
    "                    t, i_episode + 1, num_episodes, episode_rewards[i_episode], episode_rewards[i_episode - 1]))\n",
    "\n",
    "        # if terminated or truncated:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize_in_gym(agent, \"CartPole-v1\")\n",
    "# policy = agent.generate_policy_table(env.height, env.width)\n",
    "\n",
    "# print_by_dict(env, policy)\n",
    "\n",
    "# for i in range(env.height):\n",
    "#     print(\"[\", end=\" \")\n",
    "#     for j in range(env.width):\n",
    "#         state = (i,j)\n",
    "#         action = np.argmax(policy[state])\n",
    "#         print(env.action_mappings[action], end=\" \")\n",
    "#     print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\CliffWalk_A2C_solution.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_A2C_solution.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# env.max_steps = 10\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_A2C_solution.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# gridworld_demo(agent, env, repeat_times=500)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_A2C_solution.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_A2C_solution.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# visualize_in_gym(agent, \"CartPole-v1\")\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_A2C_solution.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m visualize_in_gym(agent, \u001b[39m\"\u001b[39;49m\u001b[39mCliffWalking-v0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\tools\\helper.py:121\u001b[0m, in \u001b[0;36mvisualize_in_gym\u001b[1;34m(agent, env_name, inp_env, steps)\u001b[0m\n\u001b[0;32m    116\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(\n\u001b[0;32m    117\u001b[0m     observation\n\u001b[0;32m    118\u001b[0m     , optimal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    119\u001b[0m )  \u001b[39m# agent policy that uses the observation and info\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m# insert an algorithm that can interact with env and output an action here\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m demo_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated \u001b[39mor\u001b[39;00m steps \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m99\u001b[39m:\n\u001b[0;32m    123\u001b[0m     observation, info \u001b[39m=\u001b[39m demo_env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:182\u001b[0m, in \u001b[0;36mCliffWalkingEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[0;32m    181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    183\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:207\u001b[0m, in \u001b[0;36mCliffWalkingEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:294\u001b[0m, in \u001b[0;36mCliffWalkingEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    292\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m    293\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 294\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    295\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# rgb_array\u001b[39;00m\n\u001b[0;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    297\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    298\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# env.max_steps = 10\n",
    "# gridworld_demo(agent, env, repeat_times=500)\n",
    "# gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# visualize_in_gym(agent, \"CartPole-v1\")\n",
    "visualize_in_gym(agent, \"CliffWalking-v0\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
