{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload \n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "%autoreload 2\n",
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter # type: ignore\n",
    "\n",
    "from agents.policy_gradient_cliff import PGAgent\n",
    "from tools.helper import *\n",
    "import  gymnasium  as gym\n",
    "from rl_envs.new_gym_grid_world_env import GridWorldEnv\n",
    "from torch.nn import functional as F\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 1e-2\n",
    "DISCOUNTED_FACTOR = 0.99\n",
    "\n",
    "FORBIDDEN_REWARD = -10\n",
    "HITWALL_REWARD = -10\n",
    "TARGET_REWARD = 1\n",
    "\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20fff6cec10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = GridWorldEnv(size=3,fixed_map = True, seed=SEED, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "\n",
    "# env = gym.make(\"CliffWalking-v0\")\n",
    "# torch.autograd.set_detect_anomaly(False, check_nan=True)\n",
    "# env.seed(args.seed)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_lengths = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 @ Episode 1/600 (-1.0, 0.0)\n",
      "Step 1000 @ Episode 1/600 (-3179.0, 0.0)\n",
      "Step 2000 @ Episode 1/600 (-4575.0, 0.0)\n",
      "Step 3000 @ Episode 1/600 (-5674.0, 0.0)\n",
      "Step 4000 @ Episode 1/600 (-6773.0, 0.0)\n",
      "Step 5000 @ Episode 1/600 (-7872.0, 0.0)\n",
      "Step 6000 @ Episode 1/600 (-8872.0, 0.0)\n",
      "Step 7000 @ Episode 1/600 (-9872.0, 0.0)\n",
      "Step 8000 @ Episode 1/600 (-10872.0, 0.0)\n",
      "Step 9000 @ Episode 1/600 (-11872.0, 0.0)\n",
      "Step 10000 @ Episode 1/600 (-12872.0, 0.0)\n",
      "Step 11000 @ Episode 1/600 (-13872.0, 0.0)\n",
      "Step 12000 @ Episode 1/600 (-14872.0, 0.0)\n",
      "Step 13000 @ Episode 1/600 (-15872.0, 0.0)\n",
      "Step 14000 @ Episode 1/600 (-16872.0, 0.0)\n",
      "Step 15000 @ Episode 1/600 (-17872.0, 0.0)\n",
      "Step 16000 @ Episode 1/600 (-18872.0, 0.0)\n",
      "Step 17000 @ Episode 1/600 (-19872.0, 0.0)\n",
      "Step 18000 @ Episode 1/600 (-20872.0, 0.0)\n",
      "Step 19000 @ Episode 1/600 (-21872.0, 0.0)\n",
      "Step 20000 @ Episode 1/600 (-22872.0, 0.0)\n",
      "Step 21000 @ Episode 1/600 (-23872.0, 0.0)\n",
      "Step 22000 @ Episode 1/600 (-24872.0, 0.0)\n",
      "Step 23000 @ Episode 1/600 (-25872.0, 0.0)\n",
      "Step 24000 @ Episode 1/600 (-26872.0, 0.0)\n",
      "Step 25000 @ Episode 1/600 (-27872.0, 0.0)\n",
      "Step 26000 @ Episode 1/600 (-28872.0, 0.0)\n",
      "Step 27000 @ Episode 1/600 (-30565.0, 0.0)\n",
      "Step 28000 @ Episode 1/600 (-31763.0, 0.0)\n",
      "Step 29000 @ Episode 1/600 (-32763.0, 0.0)\n",
      "Step 30000 @ Episode 1/600 (-33961.0, 0.0)\n",
      "Step 31000 @ Episode 1/600 (-34961.0, 0.0)\n",
      "Step 32000 @ Episode 1/600 (-35961.0, 0.0)\n",
      "Step 33000 @ Episode 1/600 (-36961.0, 0.0)\n",
      "Step 34000 @ Episode 1/600 (-37961.0, 0.0)\n",
      "Step 35000 @ Episode 1/600 (-38961.0, 0.0)\n",
      "Step 36000 @ Episode 1/600 (-39961.0, 0.0)\n",
      "Step 37000 @ Episode 1/600 (-41060.0, 0.0)\n",
      "Step 38000 @ Episode 1/600 (-42060.0, 0.0)\n",
      "Step 39000 @ Episode 1/600 (-43060.0, 0.0)\n",
      "Step 40000 @ Episode 1/600 (-44060.0, 0.0)\n",
      "Step 41000 @ Episode 1/600 (-45060.0, 0.0)\n",
      "Step 42000 @ Episode 1/600 (-46060.0, 0.0)\n",
      "Step 43000 @ Episode 1/600 (-47060.0, 0.0)\n",
      "Step 44000 @ Episode 1/600 (-48060.0, 0.0)\n",
      "Step 45000 @ Episode 1/600 (-49060.0, 0.0)\n",
      "Step 46000 @ Episode 1/600 (-50060.0, 0.0)\n",
      "Step 47000 @ Episode 1/600 (-51060.0, 0.0)\n",
      "Step 48000 @ Episode 1/600 (-52060.0, 0.0)\n",
      "Step 49000 @ Episode 1/600 (-53060.0, 0.0)\n",
      "Step 50000 @ Episode 1/600 (-54060.0, 0.0)\n",
      "Step 51000 @ Episode 1/600 (-55060.0, 0.0)\n",
      "Step 52000 @ Episode 1/600 (-56060.0, 0.0)\n",
      "Step 53000 @ Episode 1/600 (-57060.0, 0.0)\n",
      "Step 54000 @ Episode 1/600 (-58060.0, 0.0)\n",
      "Step 55000 @ Episode 1/600 (-59060.0, 0.0)\n",
      "Step 56000 @ Episode 1/600 (-60060.0, 0.0)\n",
      "Step 57000 @ Episode 1/600 (-61060.0, 0.0)\n",
      "Step 58000 @ Episode 1/600 (-62060.0, 0.0)\n",
      "Step 59000 @ Episode 1/600 (-63060.0, 0.0)\n",
      "Step 60000 @ Episode 1/600 (-64060.0, 0.0)\n",
      "Step 61000 @ Episode 1/600 (-65060.0, 0.0)\n",
      "Step 62000 @ Episode 1/600 (-66060.0, 0.0)\n",
      "Step 63000 @ Episode 1/600 (-67060.0, 0.0)\n",
      "Step 64000 @ Episode 1/600 (-68060.0, 0.0)\n",
      "Step 65000 @ Episode 1/600 (-69654.0, 0.0)\n",
      "Step 66000 @ Episode 1/600 (-70852.0, 0.0)\n",
      "Step 67000 @ Episode 1/600 (-72050.0, 0.0)\n",
      "Step 68000 @ Episode 1/600 (-73149.0, 0.0)\n",
      "Step 69000 @ Episode 1/600 (-74149.0, 0.0)\n",
      "Step 70000 @ Episode 1/600 (-75149.0, 0.0)\n",
      "Step 71000 @ Episode 1/600 (-76149.0, 0.0)\n",
      "Step 72000 @ Episode 1/600 (-77149.0, 0.0)\n",
      "Step 73000 @ Episode 1/600 (-78149.0, 0.0)\n",
      "Step 74000 @ Episode 1/600 (-79149.0, 0.0)\n",
      "Step 75000 @ Episode 1/600 (-80149.0, 0.0)\n",
      "Step 76000 @ Episode 1/600 (-81149.0, 0.0)\n",
      "Step 77000 @ Episode 1/600 (-82149.0, 0.0)\n",
      "Step 78000 @ Episode 1/600 (-83149.0, 0.0)\n",
      "Step 79000 @ Episode 1/600 (-84149.0, 0.0)\n",
      "Step 80000 @ Episode 1/600 (-85149.0, 0.0)\n",
      "Step 81000 @ Episode 1/600 (-86149.0, 0.0)\n",
      "Step 82000 @ Episode 1/600 (-87149.0, 0.0)\n",
      "Step 83000 @ Episode 1/600 (-88149.0, 0.0)\n",
      "Step 84000 @ Episode 1/600 (-89248.0, 0.0)\n",
      "Step 85000 @ Episode 1/600 (-90248.0, 0.0)\n",
      "Step 86000 @ Episode 1/600 (-91248.0, 0.0)\n",
      "Step 87000 @ Episode 1/600 (-92248.0, 0.0)\n",
      "Step 88000 @ Episode 1/600 (-93248.0, 0.0)\n",
      "Step 89000 @ Episode 1/600 (-94248.0, 0.0)\n",
      "Step 90000 @ Episode 1/600 (-95248.0, 0.0)\n",
      "Step 91000 @ Episode 1/600 (-96248.0, 0.0)\n",
      "Step 92000 @ Episode 1/600 (-97248.0, 0.0)\n",
      "Step 93000 @ Episode 1/600 (-98347.0, 0.0)\n",
      "Step 94000 @ Episode 1/600 (-99347.0, 0.0)\n",
      "Step 95000 @ Episode 1/600 (-100347.0, 0.0)\n",
      "Step 96000 @ Episode 1/600 (-101347.0, 0.0)\n",
      "Step 97000 @ Episode 1/600 (-102347.0, 0.0)\n",
      "Step 98000 @ Episode 1/600 (-103446.0, 0.0)\n",
      "Step 99000 @ Episode 1/600 (-104446.0, 0.0)\n",
      "Step 100000 @ Episode 1/600 (-105446.0, 0.0)\n",
      "Step 101000 @ Episode 1/600 (-106446.0, 0.0)\n",
      "Step 102000 @ Episode 1/600 (-107446.0, 0.0)\n",
      "Step 103000 @ Episode 1/600 (-108446.0, 0.0)\n",
      "Step 104000 @ Episode 1/600 (-109446.0, 0.0)\n",
      "Step 105000 @ Episode 1/600 (-110446.0, 0.0)\n",
      "Step 106000 @ Episode 1/600 (-111446.0, 0.0)\n",
      "Step 107000 @ Episode 1/600 (-112446.0, 0.0)\n",
      "Step 108000 @ Episode 1/600 (-113446.0, 0.0)\n",
      "Step 109000 @ Episode 1/600 (-114446.0, 0.0)\n",
      "Step 110000 @ Episode 1/600 (-115446.0, 0.0)\n",
      "Step 111000 @ Episode 1/600 (-116446.0, 0.0)\n",
      "Step 112000 @ Episode 1/600 (-117446.0, 0.0)\n",
      "Step 113000 @ Episode 1/600 (-118446.0, 0.0)\n",
      "Step 114000 @ Episode 1/600 (-119446.0, 0.0)\n",
      "Step 115000 @ Episode 1/600 (-120446.0, 0.0)\n",
      "Step 116000 @ Episode 1/600 (-121446.0, 0.0)\n",
      "Step 117000 @ Episode 1/600 (-122446.0, 0.0)\n",
      "Step 118000 @ Episode 1/600 (-123446.0, 0.0)\n",
      "Step 119000 @ Episode 1/600 (-124743.0, 0.0)\n",
      "Step 120000 @ Episode 1/600 (-125941.0, 0.0)\n",
      "Step 121000 @ Episode 1/600 (-126941.0, 0.0)\n",
      "Step 122000 @ Episode 1/600 (-128040.0, 0.0)\n",
      "Step 123000 @ Episode 1/600 (-129040.0, 0.0)\n",
      "Step 124000 @ Episode 1/600 (-130040.0, 0.0)\n",
      "Step 125000 @ Episode 1/600 (-131040.0, 0.0)\n",
      "Step 126000 @ Episode 1/600 (-132040.0, 0.0)\n",
      "Step 127000 @ Episode 1/600 (-133040.0, 0.0)\n",
      "Step 128000 @ Episode 1/600 (-134040.0, 0.0)\n",
      "Step 129000 @ Episode 1/600 (-135040.0, 0.0)\n",
      "Step 130000 @ Episode 1/600 (-136040.0, 0.0)\n",
      "Step 131000 @ Episode 1/600 (-137238.0, 0.0)\n",
      "Step 132000 @ Episode 1/600 (-138238.0, 0.0)\n",
      "Step 133000 @ Episode 1/600 (-139337.0, 0.0)\n",
      "Step 134000 @ Episode 1/600 (-140337.0, 0.0)\n",
      "Step 135000 @ Episode 1/600 (-141337.0, 0.0)\n",
      "Step 136000 @ Episode 1/600 (-142337.0, 0.0)\n",
      "Step 137000 @ Episode 1/600 (-143337.0, 0.0)\n",
      "Step 138000 @ Episode 1/600 (-144337.0, 0.0)\n",
      "Step 139000 @ Episode 1/600 (-145436.0, 0.0)\n",
      "Step 140000 @ Episode 1/600 (-146436.0, 0.0)\n",
      "Step 141000 @ Episode 1/600 (-147436.0, 0.0)\n",
      "Step 142000 @ Episode 1/600 (-148436.0, 0.0)\n",
      "Step 143000 @ Episode 1/600 (-149436.0, 0.0)\n",
      "Step 144000 @ Episode 1/600 (-150436.0, 0.0)\n",
      "Step 145000 @ Episode 1/600 (-151535.0, 0.0)\n",
      "Step 146000 @ Episode 1/600 (-152535.0, 0.0)\n",
      "Step 147000 @ Episode 1/600 (-153535.0, 0.0)\n",
      "Step 148000 @ Episode 1/600 (-154535.0, 0.0)\n",
      "Step 149000 @ Episode 1/600 (-155535.0, 0.0)\n",
      "Step 150000 @ Episode 1/600 (-156535.0, 0.0)\n",
      "Step 151000 @ Episode 1/600 (-157535.0, 0.0)\n",
      "Step 152000 @ Episode 1/600 (-158535.0, 0.0)\n",
      "Step 153000 @ Episode 1/600 (-159832.0, 0.0)\n",
      "Step 154000 @ Episode 1/600 (-160931.0, 0.0)\n",
      "Step 155000 @ Episode 1/600 (-161931.0, 0.0)\n",
      "Step 156000 @ Episode 1/600 (-162931.0, 0.0)\n",
      "Step 157000 @ Episode 1/600 (-164030.0, 0.0)\n",
      "Step 158000 @ Episode 1/600 (-165030.0, 0.0)\n",
      "Step 159000 @ Episode 1/600 (-166030.0, 0.0)\n",
      "Step 160000 @ Episode 1/600 (-167030.0, 0.0)\n",
      "Step 161000 @ Episode 1/600 (-168030.0, 0.0)\n",
      "Step 162000 @ Episode 1/600 (-169030.0, 0.0)\n",
      "Step 163000 @ Episode 1/600 (-170030.0, 0.0)\n",
      "Step 164000 @ Episode 1/600 (-171030.0, 0.0)\n",
      "Step 165000 @ Episode 1/600 (-172030.0, 0.0)\n",
      "Step 166000 @ Episode 1/600 (-173030.0, 0.0)\n",
      "Step 167000 @ Episode 1/600 (-174030.0, 0.0)\n",
      "Step 168000 @ Episode 1/600 (-175030.0, 0.0)\n",
      "Step 169000 @ Episode 1/600 (-176030.0, 0.0)\n",
      "Step 170000 @ Episode 1/600 (-177030.0, 0.0)\n",
      "Step 171000 @ Episode 1/600 (-178030.0, 0.0)\n",
      "Step 172000 @ Episode 1/600 (-179030.0, 0.0)\n",
      "Step 173000 @ Episode 1/600 (-180723.0, 0.0)\n",
      "Step 174000 @ Episode 1/600 (-181822.0, 0.0)\n",
      "Step 175000 @ Episode 1/600 (-182921.0, 0.0)\n",
      "Step 176000 @ Episode 1/600 (-183921.0, 0.0)\n",
      "Step 177000 @ Episode 1/600 (-184921.0, 0.0)\n",
      "Step 178000 @ Episode 1/600 (-185921.0, 0.0)\n",
      "Step 179000 @ Episode 1/600 (-186921.0, 0.0)\n",
      "Step 180000 @ Episode 1/600 (-187921.0, 0.0)\n",
      "Step 181000 @ Episode 1/600 (-188921.0, 0.0)\n",
      "Step 182000 @ Episode 1/600 (-189921.0, 0.0)\n",
      "Step 183000 @ Episode 1/600 (-190921.0, 0.0)\n",
      "Step 184000 @ Episode 1/600 (-191921.0, 0.0)\n",
      "Step 185000 @ Episode 1/600 (-192921.0, 0.0)\n",
      "Step 186000 @ Episode 1/600 (-193921.0, 0.0)\n",
      "Step 187000 @ Episode 1/600 (-195119.0, 0.0)\n",
      "Step 188000 @ Episode 1/600 (-196119.0, 0.0)\n",
      "Step 189000 @ Episode 1/600 (-197119.0, 0.0)\n",
      "Step 190000 @ Episode 1/600 (-198119.0, 0.0)\n",
      "Step 191000 @ Episode 1/600 (-199119.0, 0.0)\n",
      "Step 192000 @ Episode 1/600 (-200119.0, 0.0)\n",
      "Step 193000 @ Episode 1/600 (-201119.0, 0.0)\n",
      "Step 194000 @ Episode 1/600 (-202119.0, 0.0)\n",
      "Step 195000 @ Episode 1/600 (-203119.0, 0.0)\n",
      "Step 196000 @ Episode 1/600 (-204119.0, 0.0)\n",
      "Step 197000 @ Episode 1/600 (-205119.0, 0.0)\n",
      "Step 198000 @ Episode 1/600 (-206119.0, 0.0)\n",
      "Step 199000 @ Episode 1/600 (-207119.0, 0.0)\n",
      "Step 200000 @ Episode 1/600 (-208119.0, 0.0)\n",
      "Step 201000 @ Episode 1/600 (-209119.0, 0.0)\n",
      "Step 202000 @ Episode 1/600 (-210119.0, 0.0)\n",
      "Step 203000 @ Episode 1/600 (-211119.0, 0.0)\n",
      "Step 204000 @ Episode 1/600 (-212119.0, 0.0)\n",
      "Step 205000 @ Episode 1/600 (-213119.0, 0.0)\n",
      "Step 206000 @ Episode 1/600 (-214119.0, 0.0)\n",
      "Step 207000 @ Episode 1/600 (-215119.0, 0.0)\n",
      "Step 208000 @ Episode 1/600 (-216119.0, 0.0)\n",
      "Step 209000 @ Episode 1/600 (-217119.0, 0.0)\n",
      "Step 210000 @ Episode 1/600 (-218119.0, 0.0)\n",
      "Step 211000 @ Episode 1/600 (-219119.0, 0.0)\n",
      "Step 212000 @ Episode 1/600 (-220119.0, 0.0)\n",
      "Step 213000 @ Episode 1/600 (-221119.0, 0.0)\n",
      "Step 214000 @ Episode 1/600 (-222119.0, 0.0)\n",
      "Step 215000 @ Episode 1/600 (-223119.0, 0.0)\n",
      "Step 216000 @ Episode 1/600 (-224119.0, 0.0)\n",
      "Step 217000 @ Episode 1/600 (-225119.0, 0.0)\n",
      "Step 218000 @ Episode 1/600 (-226119.0, 0.0)\n",
      "Step 219000 @ Episode 1/600 (-227119.0, 0.0)\n",
      "Step 220000 @ Episode 1/600 (-228119.0, 0.0)\n",
      "Step 221000 @ Episode 1/600 (-229119.0, 0.0)\n",
      "Step 222000 @ Episode 1/600 (-230119.0, 0.0)\n",
      "Step 223000 @ Episode 1/600 (-231119.0, 0.0)\n",
      "Step 224000 @ Episode 1/600 (-232119.0, 0.0)\n",
      "Step 225000 @ Episode 1/600 (-233119.0, 0.0)\n",
      "Step 226000 @ Episode 1/600 (-234119.0, 0.0)\n",
      "Step 227000 @ Episode 1/600 (-235119.0, 0.0)\n",
      "Step 228000 @ Episode 1/600 (-236119.0, 0.0)\n",
      "Step 229000 @ Episode 1/600 (-237119.0, 0.0)\n",
      "Step 230000 @ Episode 1/600 (-238119.0, 0.0)\n",
      "Step 231000 @ Episode 1/600 (-239119.0, 0.0)\n",
      "Step 232000 @ Episode 1/600 (-240119.0, 0.0)\n",
      "Step 233000 @ Episode 1/600 (-241317.0, 0.0)\n",
      "Step 234000 @ Episode 1/600 (-242416.0, 0.0)\n",
      "Step 235000 @ Episode 1/600 (-243416.0, 0.0)\n",
      "Step 236000 @ Episode 1/600 (-244614.0, 0.0)\n",
      "Step 237000 @ Episode 1/600 (-245614.0, 0.0)\n",
      "Step 238000 @ Episode 1/600 (-246614.0, 0.0)\n",
      "Step 239000 @ Episode 1/600 (-247614.0, 0.0)\n",
      "Step 240000 @ Episode 1/600 (-248614.0, 0.0)\n",
      "Step 241000 @ Episode 1/600 (-249614.0, 0.0)\n",
      "Step 242000 @ Episode 1/600 (-250614.0, 0.0)\n",
      "Step 243000 @ Episode 1/600 (-251614.0, 0.0)\n",
      "Step 244000 @ Episode 1/600 (-252614.0, 0.0)\n",
      "Step 245000 @ Episode 1/600 (-253614.0, 0.0)\n",
      "Step 246000 @ Episode 1/600 (-254614.0, 0.0)\n",
      "Step 247000 @ Episode 1/600 (-255614.0, 0.0)\n",
      "Step 248000 @ Episode 1/600 (-256614.0, 0.0)\n",
      "Step 249000 @ Episode 1/600 (-257614.0, 0.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\CliffWalk_REINFORCE_solution.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# del agent.saved_log_prob\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(state) \u001b[39m# action 这里也有随机性\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     next_state, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m# next_state process HERE\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# calculate TD target\u001b[39;00m\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\agents\\policy_gradient_cliff.py:75\u001b[0m, in \u001b[0;36mPGAgent.get_action\u001b[1;34m(self, in_state, optimal)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action\u001b[39m(\u001b[39mself\u001b[39m, in_state, optimal\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     71\u001b[0m     \u001b[39m# with torch.no_grad(): # 哪里都 no_grad 只会害了你 \u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[39m# state = torch.tensor(in_state, dtype=torch.int64)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[39m# state = torch.nn.functional.one_hot(state, 48)\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     action_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net(in_state)\n\u001b[0;32m     76\u001b[0m     \u001b[39m# action_probs = (actions_val/actions_val.sum()).detach().numpy()\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[39mif\u001b[39;00m optimal:\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\agents\\policy_gradient_cliff.py:17\u001b[0m, in \u001b[0;36mPolicyNet.forward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inp):\n\u001b[0;32m     16\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(inp, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[1;32m---> 17\u001b[0m     out1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mone_hot(out, \u001b[39m48\u001b[39;49m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m     18\u001b[0m     out3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(out1)\n\u001b[0;32m     19\u001b[0m     probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(out3, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "agent = PGAgent(int(env.observation_space.n), int(env.action_space.n),  discounted_factor=1.)\n",
    "\n",
    "# env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# agent = PGAgent(2, 5, lr = LEARN_RATE, discounted_factor=DISCOUNTED_FACTOR)\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# agent = PGAgent(4, 2, lr = LEARN_RATE, discounted_factor=DISCOUNTED_FACTOR)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "num_episodes = 600\n",
    "episode_len = 10000\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "# 第一次收集改为随机收集\n",
    "trajectory = []\n",
    "obs, _ = env.reset()\n",
    "# for _ in range(1000):\n",
    "#     state = tuple(obs['agent'])\n",
    "#     action = agent.get_behavior_action(state)\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     trajectory.append((state, action, reward+10))\n",
    "running_reward = -10\n",
    "episode_rewards = defaultdict(float)\n",
    "for i_episode  in range(num_episodes):\n",
    "    # 首先, 根据 policy 生成 episode\n",
    "    next_state, _ = env.reset()\n",
    "    # trap_flag = False\n",
    "    # 初始策略是不是有比较大的影响? \n",
    "    # del agent.saved_log_probs[:]\n",
    "    for t in itertools.count():\n",
    "        # del agent.saved_log_prob\n",
    "        state = next_state\n",
    "        action = agent.get_action(state) # action 这里也有随机性\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # next_state process HERE\n",
    "\n",
    "        # calculate TD target\n",
    "        v_value = agent.value_net(state)\n",
    "        value_next = agent.value_net(next_state)\n",
    "\n",
    "        TD_target = reward + agent.discounted_factor * value_next\n",
    "        TD_error = TD_target - v_value\n",
    "\n",
    "        # Update statistics\n",
    "        episode_rewards[i_episode] += reward\n",
    "\n",
    "        agent.optimizer_v.zero_grad()\n",
    "        loss2 =  F.mse_loss(TD_target, v_value)\n",
    "        loss2.sum().backward(retain_graph=True)\n",
    "        agent.optimizer_v.step()\n",
    "\n",
    "        agent.optimizer.zero_grad()\n",
    "        loss = -agent.saved_log_prob * TD_error\n",
    "        loss.sum().backward()\n",
    "        # torch.nn.utils.clip_grad.clip_grad_norm_(agent.policy_net.parameters(), 100)\n",
    "        agent.optimizer.step()\n",
    "        if t % 1000 == 0:\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({}, {})\".format(\n",
    "                    t, i_episode + 1, num_episodes, episode_rewards[i_episode], episode_rewards[i_episode - 1]))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize_in_gym(agent, \"CartPole-v1\")\n",
    "# policy = agent.generate_policy_table(env.height, env.width)\n",
    "\n",
    "# print_by_dict(env, policy)\n",
    "\n",
    "# for i in range(env.height):\n",
    "#     print(\"[\", end=\" \")\n",
    "#     for j in range(env.width):\n",
    "#         state = (i,j)\n",
    "#         action = np.argmax(policy[state])\n",
    "#         print(env.action_mappings[action], end=\" \")\n",
    "#     print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\CliffWalk_REINFORCE_solution.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# env.max_steps = 10\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# gridworld_demo(agent, env, repeat_times=500)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# visualize_in_gym(agent, \"CartPole-v1\")\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_solution.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m visualize_in_gym(agent, \u001b[39m\"\u001b[39;49m\u001b[39mCliffWalking-v0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\tools\\helper.py:120\u001b[0m, in \u001b[0;36mvisualize_in_gym\u001b[1;34m(agent, env_name, inp_env, steps)\u001b[0m\n\u001b[0;32m    116\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(\n\u001b[0;32m    117\u001b[0m     observation, optimal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    118\u001b[0m )  \u001b[39m# agent policy that uses the observation and info\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39m# insert an algorithm that can interact with env and output an action here\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m demo_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    121\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n\u001b[0;32m    122\u001b[0m     observation, info \u001b[39m=\u001b[39m demo_env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:181\u001b[0m, in \u001b[0;36mCliffWalkingEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[0;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    182\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:206\u001b[0m, in \u001b[0;36mCliffWalkingEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[0;32m    205\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:293\u001b[0m, in \u001b[0;36mCliffWalkingEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    291\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m    292\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    294\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# rgb_array\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    296\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    297\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# env.max_steps = 10\n",
    "# gridworld_demo(agent, env, repeat_times=500)\n",
    "# gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# visualize_in_gym(agent, \"CartPole-v1\")\n",
    "visualize_in_gym(agent, \"CliffWalking-v0\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
