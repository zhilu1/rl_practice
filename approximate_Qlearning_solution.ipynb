{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload \n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "%autoreload 2\n",
    "import math\n",
    "\n",
    "from agents.approx_Q_Learning import ApproxQLearningAgent\n",
    "from tools.helper import *\n",
    "import  gymnasium  as gym\n",
    "from rl_envs.new_gym_grid_world_env import GridWorldEnv\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter # type: ignore\n",
    "import itertools\n",
    "from agents.approx_Q_Learning_cliff import CliffApproxQLearningAgent\n",
    "\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNTED_FACTOR = 0.999\n",
    "FORBIDDEN_REWARD = -10\n",
    "HITWALL_REWARD = -10\n",
    "TARGET_REWARD = 10\n",
    "NORMAL_REWARD = 0\n",
    "\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = GridWorldEnv(\n",
    "#     size=3,\n",
    "#     fixed_map=True,\n",
    "#     forbidden_grids=[(1, 1), (1, 2)],\n",
    "#     target_grids=[(2, 2)],\n",
    "#     forbidden_reward=FORBIDDEN_REWARD,\n",
    "#     hit_wall_reward=HITWALL_REWARD,\n",
    "#     target_reward=TARGET_REWARD,\n",
    "#     normal_reward=NORMAL_REWARD,\n",
    "# )\n",
    "env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD, normal_reward=NORMAL_REWARD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.action_n to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_n` for environment variables or `env.get_wrapper_attr('action_n')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CliffWalkingEnv' object has no attribute 'action_n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\approximate_Qlearning_solution.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/approximate_Qlearning_solution.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent \u001b[39m=\u001b[39m ApproxQLearningAgent(action_space_n\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39;49maction_n)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/approximate_Qlearning_solution.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m agent\u001b[39m.\u001b[39mRUN(env, num_episodes\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m, episode_len\u001b[39m=\u001b[39m\u001b[39m20000\u001b[39m)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m )\n\u001b[1;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m )\n\u001b[1;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CliffWalkingEnv' object has no attribute 'action_n'"
     ]
    }
   ],
   "source": [
    "agent = ApproxQLearningAgent(action_space_n=env.action_n)\n",
    "\n",
    "agent.RUN(env, num_episodes=100000, episode_len=20000)\n",
    "# agent.RUN(env, num_episodes=10000, episode_len=2000)\n",
    "# num_episodes=1000, episode_len=2000 能解出 3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [-0.3115143504536536, 0.15373168652040714, -9.701163625904208, -9.098343567776727, 0.6869060448910653] [-9.476055695663948, -0.37252766887615013, -11.881987367989673, 0.504187350768549, 0.5172676537618375] [-8.283195280190442, 0.25701268280685186, -9.251251770247457, -0.3862925727061258, -0.5515061735954295] [0.2433918703351825, 0.23970791447891115, -9.446275072248474, 0.04921812597993702, -0.24151862779083055] [0.2263374577724978, -9.687782353409737, -9.693326802837177, 0.20360609309668587, 0.09310984988765414] ]\n",
      "[ [-6.578993214290452, -12.078773398723179, 0.3331657189021575, -8.069392793026942, -6.945706495767432] [0.7465380445216161, -8.211677314307792, -0.938744271482185, -2.434694494312684, -8.175657938313941] [-3.3728513895807612, -0.4766312343022945, 0.5216383194818321, -12.855477457219202, -13.911099744941072] [0.13457090212703504, 0.22836794018654127, 0.21701317851239904, -9.481618533320407, 0.12815750958615868] [0.05018854115270577, -9.772494674214908, 0.20368489661270628, 0.24877134584418636, 0.18140928092124425] ]\n",
      "[ [-4.4842147398373005, -1.0171380529155976, 0.09264570068073863, -7.401319039161863, 1.458589361843226] [-9.432071628578136, -8.410362197653097, -9.547838911319806, 1.0297807331101498, 0.9045278070613664] [-6.003363058020659, -2.6081316428732233, -5.402199654021772, 1.849907826537006, -0.5941135505046693] [-10.913578697929498, -0.5164469161357568, 0.1654225357265252, -7.317222597669734, 0.10922798367755988] [-2.0989359727336336, -12.057877887580002, 0.10754423055723794, 0.11776482797306419, -1.219759339374042] ]\n",
      "[ [-2.136432956987064, -7.742673459409392, -5.580836099407638, -12.66777616167383, -2.1616526962487272] [-7.652528835045597, -0.33390150454811696, 0.945947687575655, 0.6184200161379052, -11.15684131731313] [-14.345917776739732, -14.10019004360311, -17.74273033262958, -14.09154625416985, -15.154134977371159] [-0.5853879275739678, -0.42464601645515154, -5.84403555837706, -5.452220755351713, -6.8189417098337515] [-0.1895759792687406, -10.636032804144026, -0.26206420224195426, -10.669435626876002, -0.17260197304791058] ]\n",
      "[ [-9.122456245262448, -6.094654874235123, -2.251430817356619, -12.193185147214965, -1.3597504016596353] [-8.862337288966053, 2.846678890381387, -6.17928345459789, 2.1036724090135217, -7.15458102701918] [-8.897866210754737, 0.10785399529938455, -3.3269652624381267, -6.043822132523925, 1.9575646643895976] [-7.214162812489032, -0.37907823805232527, -10.700574198547972, -7.140109759259976, -4.082541439820457] [-9.684849141601012, -10.066154442999837, -0.43299565644963933, -1.0142273759454603, 0.023232819748234435] ]\n"
     ]
    }
   ],
   "source": [
    "for y in range(env.height):\n",
    "    for x in range(env.width):\n",
    "        agent.policy_improvement((y, x))\n",
    "\n",
    "Q = defaultdict(lambda: [0 for _ in range(agent.action_space_n)])\n",
    "for y in range(env.height):\n",
    "    for x in range(env.width):\n",
    "        for action in range(agent.action_space_n):\n",
    "            state = (y,x)\n",
    "            Q[state][action] = agent.estimate_q(state, action)\n",
    "print_by_dict(env, Q)\n",
    "\n",
    "# ISSUE: 在 X 相同时, 不同的 y, q 值保持一致 (为何)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [0.02 0.02 0.02 0.02 0.92] [0.02 0.02 0.02 0.02 0.92] [0.02 0.92 0.02 0.02 0.02] [0.92 0.02 0.02 0.02 0.02] [0.92 0.02 0.02 0.02 0.02] ]\n",
      "[ [0.02 0.02 0.92 0.02 0.02] [0.92 0.02 0.02 0.02 0.02] [0.02 0.02 0.92 0.02 0.02] [0.02 0.92 0.02 0.02 0.02] [0.02 0.02 0.02 0.92 0.02] ]\n",
      "[ [0.02 0.02 0.02 0.02 0.92] [0.02 0.02 0.02 0.92 0.02] [0.02 0.02 0.02 0.92 0.02] [0.02 0.02 0.92 0.02 0.02] [0.02 0.02 0.02 0.92 0.02] ]\n",
      "[ [0.92 0.02 0.02 0.02 0.02] [0.02 0.02 0.92 0.02 0.02] [0.02 0.02 0.02 0.92 0.02] [0.02 0.92 0.02 0.02 0.02] [0.02 0.02 0.02 0.02 0.92] ]\n",
      "[ [0.02 0.02 0.02 0.02 0.92] [0.02 0.92 0.02 0.02 0.02] [0.02 0.02 0.02 0.02 0.92] [0.02 0.92 0.02 0.02 0.02] [0.02 0.02 0.02 0.02 0.92] ]\n",
      "\n",
      "[-4.57554407  0.55017997  0.72047626 -0.69717129 -1.6892028  -0.7497669\n",
      "  0.11583587  0.24043409 -0.70021934  0.0914042   2.15549114  0.89401434\n",
      "  1.71693936  1.67135583 -0.43977293 -0.13612851 -0.53958727  0.97631375\n",
      "  1.71196945 -1.071385   -1.411142    0.43318891 -0.52883288 -0.61320027\n",
      " -2.47908842  0.89997364  0.94686913  1.24314935  0.18778291 -1.15774433\n",
      " -1.36735804  0.28237404 -0.10239134  0.13543175 -0.02306565 -0.61547153\n",
      "  1.52152487  0.62842133 -1.64259446 -0.24892333  0.40752706 -0.22761255\n",
      "  1.42361885 -1.07737898 -0.47188288  0.59877482 -1.92640551  0.47408874\n",
      "  1.32069198 -0.6147486  -1.58795543 -0.74275714  0.29134409  0.44931083\n",
      " -0.24706971  0.29506623 -0.66414565 -1.47904199  0.55119059 -0.27007847\n",
      " -0.38749085  1.80682312  0.61227626  0.63469034 -0.5819967  -2.38172492\n",
      "  0.03400326  1.91270962  0.59154491 -0.84572133 -1.60658119 -0.94918574\n",
      "  1.64938113 -0.07484847  1.71815224  1.10404403 -1.42216025  0.89579549\n",
      " -0.86984634 -1.07968721  1.10738916  0.5867759  -0.94450016  0.09506234\n",
      " -0.0524215  -1.79318034  0.07421341 -0.42973989  1.14466244  0.47095282\n",
      "  0.33957201  0.43055366 -0.50281136 -0.61285308  0.83557757  0.57230393\n",
      "  0.92786254 -0.25794433 -0.3449645  -0.30011387  0.38922353  0.39494319\n",
      "  0.57035665 -0.80290896  0.82980175  1.31066467  0.90024471 -1.2830797\n",
      " -1.13577146  1.09378488  3.00547673  0.50749749 -0.14112985 -0.70042152\n",
      " -0.44013619  0.45002286  0.66542004  0.01571867  1.24504351 -1.26222719\n",
      " -0.35443906 -0.11762344  0.42263945  0.03674329 -0.85500865]\n",
      "[ 0.00 0.00 0.00 0.00 0.00 ]\n",
      "[ 0.00 -10.00 -10.00 0.00 0.00 ]\n",
      "[ 0.00 0.00 -10.00 0.00 0.00 ]\n",
      "[ 0.00 -10.00 10.00 -10.00 0.00 ]\n",
      "[ 0.00 -10.00 0.00 0.00 0.00 ]\n",
      "\n",
      "[  ↺   ↺   →   ↓   ↓  ]\n",
      "[  ↑   ↓   ↑   →   ←  ]\n",
      "[  ↺   ←   ←   ↑   ←  ]\n",
      "[  ↓   ↑   ←   →   ↺  ]\n",
      "[  ↺   →   ↺   →   ↺  ]\n"
     ]
    }
   ],
   "source": [
    "print_by_dict(env, agent.policy)\n",
    "print()\n",
    "print(agent.parameters)\n",
    "\n",
    "print(env)\n",
    "for i in range(env.height):\n",
    "    print(\"[\", end=\" \")\n",
    "    for j in range(env.width):\n",
    "        state = (i, j)\n",
    "        action = agent.get_action(state, optimal=True)\n",
    "        print(env.action_mappings[action], end=\" \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 @ Episode 1/100000 (-100.0, 0.0)\n",
      " len 0 Episode 0/100000 ( TD_error: -100.0, reward: -100.0,0.0)\n",
      "Step 0 @ Episode 2/100000 (-100.0, -100.0)\n",
      "Step 0 @ Episode 3/100000 (-1.0, -100.0)\n",
      "Step 0 @ Episode 4/100000 (-1.0, -123.0)\n",
      "Step 0 @ Episode 5/100000 (-1.0, -120.0)\n",
      "Step 0 @ Episode 6/100000 (-1.0, -128.0)\n",
      "Step 0 @ Episode 7/100000 (-1.0, -111.0)\n",
      "Step 0 @ Episode 8/100000 (-1.0, -115.0)\n",
      "Step 0 @ Episode 9/100000 (-1.0, -117.0)\n",
      "Step 0 @ Episode 10/100000 (-1.0, -241.0)\n",
      "Step 0 @ Episode 11/100000 (-1.0, -120.0)\n",
      "Step 0 @ Episode 12/100000 (-1.0, -275.0)\n",
      "Step 0 @ Episode 13/100000 (-1.0, -146.0)\n",
      "Step 0 @ Episode 14/100000 (-1.0, -193.0)\n",
      "Step 0 @ Episode 15/100000 (-1.0, -385.0)\n",
      "Step 0 @ Episode 16/100000 (-1.0, -114.0)\n",
      "Step 0 @ Episode 17/100000 (-1.0, -131.0)\n",
      "Step 0 @ Episode 18/100000 (-1.0, -157.0)\n",
      "Step 0 @ Episode 19/100000 (-1.0, -339.0)\n",
      "Step 0 @ Episode 20/100000 (-1.0, -369.0)\n",
      "Step 0 @ Episode 21/100000 (-1.0, -142.0)\n",
      "Step 0 @ Episode 22/100000 (-1.0, -285.0)\n",
      "Step 0 @ Episode 23/100000 (-1.0, -102.0)\n",
      "Step 0 @ Episode 24/100000 (-1.0, -110.0)\n",
      "Step 0 @ Episode 25/100000 (-1.0, -134.0)\n",
      "Step 0 @ Episode 26/100000 (-1.0, -162.0)\n",
      "Step 0 @ Episode 27/100000 (-1.0, -271.0)\n",
      "Step 0 @ Episode 28/100000 (-1.0, -195.0)\n",
      "Step 0 @ Episode 29/100000 (-1.0, -292.0)\n",
      "Step 0 @ Episode 30/100000 (-1.0, -129.0)\n",
      "Step 0 @ Episode 31/100000 (-1.0, -273.0)\n",
      "Step 0 @ Episode 32/100000 (-1.0, -140.0)\n",
      "Step 0 @ Episode 33/100000 (-1.0, -197.0)\n",
      "Step 0 @ Episode 34/100000 (-1.0, -161.0)\n",
      "Step 0 @ Episode 35/100000 (-1.0, -309.0)\n",
      "Step 0 @ Episode 36/100000 (-1.0, -252.0)\n",
      "Step 0 @ Episode 37/100000 (-1.0, -132.0)\n",
      "Step 0 @ Episode 38/100000 (-1.0, -383.0)\n",
      "Step 0 @ Episode 39/100000 (-1.0, -302.0)\n",
      "Step 0 @ Episode 40/100000 (-1.0, -328.0)\n",
      "Step 0 @ Episode 41/100000 (-1.0, -106.0)\n",
      "Step 0 @ Episode 42/100000 (-1.0, -147.0)\n",
      "Step 0 @ Episode 43/100000 (-1.0, -215.0)\n",
      "Step 0 @ Episode 44/100000 (-1.0, -283.0)\n",
      "Step 1000 @ Episode 44/100000 (-1001.0, -283.0)\n",
      "Step 0 @ Episode 45/100000 (-1.0, -1828.0)\n",
      "Step 0 @ Episode 46/100000 (-1.0, -229.0)\n",
      "Step 0 @ Episode 47/100000 (-1.0, -607.0)\n",
      "Step 0 @ Episode 48/100000 (-1.0, -185.0)\n",
      "Step 0 @ Episode 49/100000 (-1.0, -124.0)\n",
      "Step 0 @ Episode 50/100000 (-1.0, -113.0)\n",
      "Step 0 @ Episode 51/100000 (-1.0, -125.0)\n",
      "Step 1000 @ Episode 51/100000 (-1001.0, -125.0)\n",
      "Step 0 @ Episode 52/100000 (-1.0, -1117.0)\n",
      "Step 0 @ Episode 53/100000 (-1.0, -288.0)\n",
      "Step 0 @ Episode 54/100000 (-1.0, -446.0)\n",
      "Step 0 @ Episode 55/100000 (-1.0, -114.0)\n",
      "Step 1000 @ Episode 55/100000 (-1001.0, -114.0)\n",
      "Step 0 @ Episode 56/100000 (-1.0, -1660.0)\n",
      "Step 0 @ Episode 57/100000 (-1.0, -259.0)\n",
      "Step 0 @ Episode 58/100000 (-1.0, -189.0)\n",
      "Step 0 @ Episode 59/100000 (-1.0, -498.0)\n",
      "Step 0 @ Episode 60/100000 (-1.0, -237.0)\n",
      "Step 0 @ Episode 61/100000 (-1.0, -230.0)\n",
      "Step 0 @ Episode 62/100000 (-1.0, -353.0)\n",
      "Step 0 @ Episode 63/100000 (-1.0, -184.0)\n",
      "Step 0 @ Episode 64/100000 (-1.0, -336.0)\n",
      "Step 0 @ Episode 65/100000 (-1.0, -225.0)\n",
      "Step 0 @ Episode 66/100000 (-1.0, -147.0)\n",
      "Step 0 @ Episode 67/100000 (-1.0, -121.0)\n",
      "Step 0 @ Episode 68/100000 (-1.0, -148.0)\n",
      "Step 0 @ Episode 69/100000 (-1.0, -364.0)\n",
      "Step 0 @ Episode 70/100000 (-1.0, -988.0)\n",
      "Step 0 @ Episode 71/100000 (-1.0, -1019.0)\n",
      "Step 0 @ Episode 72/100000 (-1.0, -314.0)\n",
      "Step 0 @ Episode 73/100000 (-1.0, -608.0)\n",
      "Step 0 @ Episode 74/100000 (-1.0, -246.0)\n",
      "Step 1000 @ Episode 74/100000 (-1001.0, -246.0)\n",
      "Step 0 @ Episode 75/100000 (-1.0, -1243.0)\n",
      "Step 0 @ Episode 76/100000 (-1.0, -782.0)\n",
      "Step 1000 @ Episode 76/100000 (-1001.0, -782.0)\n",
      "Step 0 @ Episode 77/100000 (-1.0, -1644.0)\n",
      "Step 0 @ Episode 78/100000 (-1.0, -333.0)\n",
      "Step 1000 @ Episode 78/100000 (-1001.0, -333.0)\n",
      "Step 0 @ Episode 79/100000 (-1.0, -1849.0)\n",
      "Step 0 @ Episode 80/100000 (-1.0, -524.0)\n",
      "Step 0 @ Episode 81/100000 (-1.0, -170.0)\n",
      "Step 0 @ Episode 82/100000 (-1.0, -201.0)\n",
      "Step 1000 @ Episode 82/100000 (-1001.0, -201.0)\n",
      "Step 2000 @ Episode 82/100000 (-2001.0, -201.0)\n",
      "Step 3000 @ Episode 82/100000 (-3001.0, -201.0)\n",
      "Step 0 @ Episode 83/100000 (-1.0, -3339.0)\n",
      "Step 1000 @ Episode 83/100000 (-1001.0, -3339.0)\n",
      "Step 0 @ Episode 84/100000 (-1.0, -1295.0)\n",
      "Step 0 @ Episode 85/100000 (-1.0, -1036.0)\n",
      "Step 0 @ Episode 86/100000 (-1.0, -893.0)\n",
      "Step 0 @ Episode 87/100000 (-1.0, -671.0)\n",
      "Step 1000 @ Episode 87/100000 (-1001.0, -671.0)\n",
      "Step 0 @ Episode 88/100000 (-1.0, -1288.0)\n",
      "Step 1000 @ Episode 88/100000 (-1001.0, -1288.0)\n",
      "Step 2000 @ Episode 88/100000 (-2001.0, -1288.0)\n",
      "Step 3000 @ Episode 88/100000 (-3001.0, -1288.0)\n",
      "Step 4000 @ Episode 88/100000 (-4001.0, -1288.0)\n",
      "Step 0 @ Episode 89/100000 (-1.0, -4291.0)\n",
      "Step 0 @ Episode 90/100000 (-1.0, -283.0)\n",
      "Step 1000 @ Episode 90/100000 (-1001.0, -283.0)\n",
      "Step 2000 @ Episode 90/100000 (-2001.0, -283.0)\n",
      "Step 0 @ Episode 91/100000 (-1.0, -2270.0)\n",
      "Step 0 @ Episode 92/100000 (-1.0, -226.0)\n",
      "Step 1000 @ Episode 92/100000 (-1001.0, -226.0)\n",
      "Step 0 @ Episode 93/100000 (-1.0, -1235.0)\n",
      "Step 1000 @ Episode 93/100000 (-1001.0, -1235.0)\n",
      "Step 2000 @ Episode 93/100000 (-2001.0, -1235.0)\n",
      "Step 0 @ Episode 94/100000 (-1.0, -2603.0)\n",
      "Step 1000 @ Episode 94/100000 (-1001.0, -2603.0)\n",
      "Step 0 @ Episode 95/100000 (-1.0, -1309.0)\n",
      "Step 1000 @ Episode 95/100000 (-1001.0, -1309.0)\n",
      "Step 2000 @ Episode 95/100000 (-2001.0, -1309.0)\n",
      "Step 3000 @ Episode 95/100000 (-3001.0, -1309.0)\n",
      "Step 4000 @ Episode 95/100000 (-4001.0, -1309.0)\n",
      "Step 5000 @ Episode 95/100000 (-5001.0, -1309.0)\n",
      "Step 6000 @ Episode 95/100000 (-6001.0, -1309.0)\n",
      "Step 7000 @ Episode 95/100000 (-7001.0, -1309.0)\n",
      "Step 8000 @ Episode 95/100000 (-8001.0, -1309.0)\n",
      "Step 9000 @ Episode 95/100000 (-9001.0, -1309.0)\n",
      "Step 10000 @ Episode 95/100000 (-10001.0, -1309.0)\n",
      "Step 11000 @ Episode 95/100000 (-11001.0, -1309.0)\n",
      "Step 12000 @ Episode 95/100000 (-12001.0, -1309.0)\n",
      "Step 13000 @ Episode 95/100000 (-13001.0, -1309.0)\n",
      "Step 14000 @ Episode 95/100000 (-14001.0, -1309.0)\n",
      "Step 15000 @ Episode 95/100000 (-15001.0, -1309.0)\n",
      "Step 16000 @ Episode 95/100000 (-16001.0, -1309.0)\n",
      "Step 17000 @ Episode 95/100000 (-17001.0, -1309.0)\n",
      "Step 18000 @ Episode 95/100000 (-18001.0, -1309.0)\n",
      "Step 19000 @ Episode 95/100000 (-19001.0, -1309.0)\n",
      "Step 20000 @ Episode 95/100000 (-20001.0, -1309.0)\n",
      "Step 21000 @ Episode 95/100000 (-21001.0, -1309.0)\n",
      "Step 22000 @ Episode 95/100000 (-22001.0, -1309.0)\n",
      "Step 0 @ Episode 96/100000 (-1.0, -22509.0)\n",
      "Step 1000 @ Episode 96/100000 (-1001.0, -22509.0)\n",
      "Step 2000 @ Episode 96/100000 (-2001.0, -22509.0)\n",
      "Step 3000 @ Episode 96/100000 (-3001.0, -22509.0)\n",
      "Step 0 @ Episode 97/100000 (-1.0, -3395.0)\n",
      "Step 0 @ Episode 98/100000 (-1.0, -167.0)\n",
      "Step 1000 @ Episode 98/100000 (-1001.0, -167.0)\n",
      "Step 2000 @ Episode 98/100000 (-2001.0, -167.0)\n",
      "Step 3000 @ Episode 98/100000 (-3001.0, -167.0)\n",
      "Step 4000 @ Episode 98/100000 (-4001.0, -167.0)\n",
      "Step 0 @ Episode 99/100000 (-1.0, -4213.0)\n",
      "Step 1000 @ Episode 99/100000 (-1001.0, -4213.0)\n",
      "Step 2000 @ Episode 99/100000 (-2001.0, -4213.0)\n",
      "Step 3000 @ Episode 99/100000 (-3001.0, -4213.0)\n",
      "Step 4000 @ Episode 99/100000 (-4001.0, -4213.0)\n",
      "Step 5000 @ Episode 99/100000 (-5001.0, -4213.0)\n",
      "Step 6000 @ Episode 99/100000 (-6001.0, -4213.0)\n",
      "Step 7000 @ Episode 99/100000 (-7001.0, -4213.0)\n",
      "Step 8000 @ Episode 99/100000 (-8001.0, -4213.0)\n",
      "Step 9000 @ Episode 99/100000 (-9001.0, -4213.0)\n",
      "Step 0 @ Episode 100/100000 (-1.0, -9613.0)\n",
      "Step 1000 @ Episode 100/100000 (-1001.0, -9613.0)\n",
      "Step 0 @ Episode 101/100000 (-1.0, -1788.0)\n",
      " len 269 Episode 100/100000 ( TD_error: -80.4944037930463, reward: -369.0,-1788.0)\n",
      "Step 0 @ Episode 102/100000 (-1.0, -369.0)\n",
      "Step 1000 @ Episode 102/100000 (-1001.0, -369.0)\n",
      "Step 2000 @ Episode 102/100000 (-2001.0, -369.0)\n",
      "Step 3000 @ Episode 102/100000 (-3001.0, -369.0)\n",
      "Step 4000 @ Episode 102/100000 (-4001.0, -369.0)\n",
      "Step 5000 @ Episode 102/100000 (-5001.0, -369.0)\n",
      "Step 6000 @ Episode 102/100000 (-6001.0, -369.0)\n",
      "Step 7000 @ Episode 102/100000 (-7001.0, -369.0)\n",
      "Step 8000 @ Episode 102/100000 (-8001.0, -369.0)\n",
      "Step 0 @ Episode 103/100000 (-1.0, -8285.0)\n",
      "Step 0 @ Episode 104/100000 (-1.0, -911.0)\n",
      "Step 0 @ Episode 105/100000 (-1.0, -854.0)\n",
      "Step 1000 @ Episode 105/100000 (-1001.0, -854.0)\n",
      "Step 2000 @ Episode 105/100000 (-2001.0, -854.0)\n",
      "Step 3000 @ Episode 105/100000 (-3001.0, -854.0)\n",
      "Step 0 @ Episode 106/100000 (-1.0, -3479.0)\n",
      "Step 0 @ Episode 107/100000 (-1.0, -239.0)\n",
      "Step 0 @ Episode 108/100000 (-1.0, -1023.0)\n",
      "Step 0 @ Episode 109/100000 (-1.0, -230.0)\n",
      "Step 1000 @ Episode 109/100000 (-1001.0, -230.0)\n",
      "Step 0 @ Episode 110/100000 (-1.0, -1168.0)\n",
      "Step 0 @ Episode 111/100000 (-1.0, -186.0)\n",
      "Step 1000 @ Episode 111/100000 (-1001.0, -186.0)\n",
      "Step 2000 @ Episode 111/100000 (-2001.0, -186.0)\n",
      "Step 3000 @ Episode 111/100000 (-3001.0, -186.0)\n",
      "Step 4000 @ Episode 111/100000 (-4001.0, -186.0)\n",
      "Step 0 @ Episode 112/100000 (-1.0, -4278.0)\n",
      "Step 0 @ Episode 113/100000 (-1.0, -413.0)\n",
      "Step 1000 @ Episode 113/100000 (-1001.0, -413.0)\n",
      "Step 0 @ Episode 114/100000 (-1.0, -1429.0)\n",
      "Step 0 @ Episode 115/100000 (-1.0, -131.0)\n",
      "Step 0 @ Episode 116/100000 (-1.0, -507.0)\n",
      "Step 0 @ Episode 117/100000 (-1.0, -263.0)\n",
      "Step 0 @ Episode 118/100000 (-1.0, -276.0)\n",
      "Step 1000 @ Episode 118/100000 (-1001.0, -276.0)\n",
      "Step 0 @ Episode 119/100000 (-1.0, -1309.0)\n",
      "Step 0 @ Episode 120/100000 (-1.0, -1004.0)\n",
      "Step 0 @ Episode 121/100000 (-1.0, -131.0)\n",
      "Step 1000 @ Episode 121/100000 (-1001.0, -131.0)\n",
      "Step 0 @ Episode 122/100000 (-1.0, -1637.0)\n",
      "Step 1000 @ Episode 122/100000 (-1001.0, -1637.0)\n",
      "Step 2000 @ Episode 122/100000 (-2001.0, -1637.0)\n",
      "Step 0 @ Episode 123/100000 (-1.0, -2234.0)\n",
      "Step 1000 @ Episode 123/100000 (-1001.0, -2234.0)\n",
      "Step 2000 @ Episode 123/100000 (-2001.0, -2234.0)\n",
      "Step 3000 @ Episode 123/100000 (-3001.0, -2234.0)\n",
      "Step 4000 @ Episode 123/100000 (-4001.0, -2234.0)\n",
      "Step 5000 @ Episode 123/100000 (-5001.0, -2234.0)\n",
      "Step 6000 @ Episode 123/100000 (-6001.0, -2234.0)\n",
      "Step 7000 @ Episode 123/100000 (-7001.0, -2234.0)\n",
      "Step 0 @ Episode 124/100000 (-1.0, -8022.0)\n",
      "Step 1000 @ Episode 124/100000 (-1001.0, -8022.0)\n",
      "Step 2000 @ Episode 124/100000 (-2001.0, -8022.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\approximate_Qlearning_solution.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/approximate_Qlearning_solution.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCliffWalking-v0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/approximate_Qlearning_solution.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m agent \u001b[39m=\u001b[39m CliffApproxQLearningAgent(action_space_n\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn, discounted_factor\u001b[39m=\u001b[39m\u001b[39m0.99999\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/approximate_Qlearning_solution.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m agent\u001b[39m.\u001b[39;49mRUN(env, num_episodes\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m, episode_len\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m)\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\agents\\approx_Q_Learning_cliff.py:174\u001b[0m, in \u001b[0;36mCliffApproxQLearningAgent.RUN\u001b[1;34m(self, env, num_episodes, episode_len)\u001b[0m\n\u001b[0;32m    172\u001b[0m best_action \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    173\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space_n):\n\u001b[1;32m--> 174\u001b[0m     q_eval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimate_q(next_state, a)\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;00m q_eval \u001b[39m>\u001b[39m next_q:\n\u001b[0;32m    176\u001b[0m         next_q \u001b[39m=\u001b[39m q_eval\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\agents\\approx_Q_Learning_cliff.py:43\u001b[0m, in \u001b[0;36mCliffApproxQLearningAgent.estimate_q\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     41\u001b[0m s, z \u001b[39m=\u001b[39m state, action\n\u001b[0;32m     42\u001b[0m \u001b[39m# s, z = (state)/96, (action)/6\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m phi_mat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mcos((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mphi_ls \u001b[39m@\u001b[39;49m np\u001b[39m.\u001b[39;49marray([s,z]))\u001b[39m*\u001b[39;49mpi)\n\u001b[0;32m     44\u001b[0m q \u001b[39m=\u001b[39m phi_mat \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters\n\u001b[0;32m     45\u001b[0m \u001b[39m# for i, param in enumerate(self.parameters):\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m#     q += self.phi_sa(state, action, i) * param\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "agent = CliffApproxQLearningAgent(action_space_n=env.action_space.n, discounted_factor=0.99999)\n",
    "\n",
    "agent.RUN(env, num_episodes=100000, episode_len=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "↑ → → → ↓ ← ← ← → ↓ ↑ ↑ \n",
      "↑ → ← ↓ ← ← ← ↑ ← ← ↑ → \n",
      "↓ ← ↓ ↑ → ↓ ↓ ↓ ← ↓ ← → \n",
      "← ↑ ↑ ← ← → ↑ ↑ ← ← → ↓ "
     ]
    }
   ],
   "source": [
    "action_mappings = [\"↑\",\"→\",\"↓\",\"←\"]\n",
    "for i in range(48):\n",
    "    if i % 12 == 0:\n",
    "        print()\n",
    "    action = agent.get_action(i)\n",
    "    print(action_mappings[action], end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
