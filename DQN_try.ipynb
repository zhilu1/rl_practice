{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from rl_envs.gym_grid_world_env import GridWorldEnv\n",
    "import numpy as np\n",
    "from tools.helper import *\n",
    "# from rl_envs.episodic_grid_world_env import EpisodicGridWorldEnv\n",
    "# from rl_envs.grid_world_env import GridWorldEnv\n",
    "from ReplayMemory import *\n",
    "from agents.DQN import DeepQLearningAgent\n",
    "%load_ext autoreload \n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_actions(agent, env, get_optimal = False):\n",
    "    with torch.no_grad():\n",
    "        action_mapping = [\" ↓ \",\" ↑ \",\" → \",\" ← \",\" ↺ \"]\n",
    "        for i in range(env.height):\n",
    "            print(\"[\", end=\" \")\n",
    "            for j in range(env.width):\n",
    "                state = torch.tensor((i,j), dtype=torch.float).unsqueeze(0)\n",
    "                action = agent.get_action(state)\n",
    "                print(action_mapping[action.item()], end=\" \")\n",
    "            print(\"]\")\n",
    "\n",
    "def state_normalize(env, state):\n",
    "    return ((state[0] - (env.height-1)/2.0)/env.height,(state[1] - (env.width-1)/2.0)/env.width)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BATCHSIZE = 100\n",
    "LEARN_RATE = 1e-4 # change to 1e-4\n",
    "TRUE_RANDOM_STATE_VALUE = [\n",
    "    [-3.8, -3.8, -3.6, -3.1, -3.2],\n",
    "    [-3.8, -3.8, -3.8, -3.1, -2.9],\n",
    "    [-3.6, -3.9, -3.4, -3.2, -2.9],\n",
    "    [-3.9, -3.6, -3.4, -2.9, -3.2],\n",
    "    [-4.5, -4.2, -3.4, -3.4, -3.5],         \n",
    "]\n",
    "\n",
    "def calculate_state_value_error(env:GridWorldEnv,agent):\n",
    "    # offline policy have 2 policies, I am using the behavior(random) policy for calculating\n",
    "    with torch.no_grad():\n",
    "        state_value_error = 0\n",
    "        for i in range(env.height):\n",
    "            for j in range(env.width):\n",
    "                state = torch.tensor((i,j), dtype=torch.float).unsqueeze(0)\n",
    "                output = agent.policy_net(state)\n",
    "                state_value = output.sum()/env.action_n\n",
    "                state_value_error += (state_value - TRUE_RANDOM_STATE_VALUE[i][j])\n",
    "    return state_value_error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=-1, hit_wall_reward=-1, target_reward=10)\n",
    "agent = DeepQLearningAgent(state_space_n= 2, action_space_n=env.action_n, lr = LEARN_RATE)\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generate samples to replay buffer\n",
    "\"\"\"\n",
    "replay_buffer = ReplayMemory(10000000)\n",
    "\n",
    "episode_num = 1000000\n",
    "for _ in range(episode_num):\n",
    "    state, info = env.reset()\n",
    "    for _ in range(100):\n",
    "        action = random.randint(0,int(env.action_n)-1)\n",
    "        # action = agent.get_behavior_acion(state)\n",
    "        next_state, reward, terminated , truncated, info = env.step(action)\n",
    "        # replay_buffer.push(torch.tensor(state_normalize(env,state), dtype=torch.float), torch.tensor(action, dtype=torch.int64).unsqueeze(0), torch.tensor(reward, dtype=torch.float).unsqueeze(0), torch.tensor(state_normalize(env,next_state), dtype=torch.float))\n",
    "        replay_buffer.push(torch.tensor(state['agent'], dtype=torch.float), torch.tensor(action, dtype=torch.int64).unsqueeze(0), torch.tensor(reward, dtype=torch.float).unsqueeze(0), torch.tensor(next_state['agent'], dtype=torch.float))\n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "perform DQN\n",
    "\"\"\"\n",
    "# iter_counter = 0\n",
    "# q_value = target_value = loss = []\n",
    "for iter_counter in range(1000000):\n",
    "    transitions  = replay_buffer.sample(BATCHSIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state = torch.stack(batch.state)\n",
    "    next_state = torch.stack(batch.next_state)\n",
    "    reward = torch.cat(batch.reward)\n",
    "    action_indices = torch.cat(batch.action)\n",
    "    \n",
    "    loss, q_value, target_value = agent.update_Q_network(state, action_indices, reward, next_state)\n",
    "\n",
    "    if iter_counter % 50 == 0:\n",
    "        # copy target network every C=50 iteration\n",
    "        # state_value_estimated = output.sum(dim=1) / env.possible_actions \n",
    "        writer.add_scalar('TD error', (q_value - target_value).sum(), iter_counter)         \n",
    "        writer.add_scalar('Loss', loss.sum(), iter_counter)\n",
    "        writer.add_scalar('State value error', calculate_state_value_error(env,agent), iter_counter)\n",
    "\n",
    "\n",
    "        # iter_counter+=1\n",
    "        # agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "        agent.sync_target_network()\n",
    "    # print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GridWorldEnv instance>\n",
      "[  ←   ←   ←   ↓   ↓  ]\n",
      "[  ←   ←   ←   ↓   ↓  ]\n",
      "[  ←   ←   ↑   ↑   ↓  ]\n",
      "[  ←   ←   ↑   ↑   ↑  ]\n",
      "[  ←   ←   ↑   ↑   ↑  ]\n",
      "\n",
      "[ tensor(3.7849, grad_fn=<SubBackward0>) tensor(3.8066, grad_fn=<SubBackward0>) tensor(3.6185, grad_fn=<SubBackward0>) tensor(3.1156, grad_fn=<SubBackward0>) tensor(3.2161, grad_fn=<SubBackward0>) ]\n",
      "[ tensor(3.7790, grad_fn=<SubBackward0>) tensor(3.7925, grad_fn=<SubBackward0>) tensor(3.7787, grad_fn=<SubBackward0>) tensor(3.0724, grad_fn=<SubBackward0>) tensor(2.8702, grad_fn=<SubBackward0>) ]\n",
      "[ tensor(3.5921, grad_fn=<SubBackward0>) tensor(3.8787, grad_fn=<SubBackward0>) tensor(3.3621, grad_fn=<SubBackward0>) tensor(3.1422, grad_fn=<SubBackward0>) tensor(2.8398, grad_fn=<SubBackward0>) ]\n",
      "[ tensor(3.9038, grad_fn=<SubBackward0>) tensor(3.5691, grad_fn=<SubBackward0>) tensor(3.3587, grad_fn=<SubBackward0>) tensor(2.8253, grad_fn=<SubBackward0>) tensor(3.1060, grad_fn=<SubBackward0>) ]\n",
      "[ tensor(4.5127, grad_fn=<SubBackward0>) tensor(4.1706, grad_fn=<SubBackward0>) tensor(3.3448, grad_fn=<SubBackward0>) tensor(3.3262, grad_fn=<SubBackward0>) tensor(3.3945, grad_fn=<SubBackward0>) ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "print(env)\n",
    "\n",
    "print_actions(agent, env, True)\n",
    "\n",
    "print()\n",
    "\n",
    "for i in range(env.height):\n",
    "    print(\"[\", end=\" \")\n",
    "    for j in range(env.width):\n",
    "        state = torch.tensor((i,j), dtype=torch.float).unsqueeze(0)\n",
    "        output = agent.policy_net(state)\n",
    "        state_value = output.sum()/env.action_n\n",
    "        state_value_error = (state_value - TRUE_RANDOM_STATE_VALUE[i][j])\n",
    "        print(state_value_error, end=\" \")\n",
    "    print(\"]\")\n",
    "\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\DQN_try.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/DQN_try.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(env\u001b[39m.\u001b[39msize):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/DQN_try.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m        state \u001b[39m=\u001b[39m (y,x)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/DQN_try.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m        q_values \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mpolicy_net(state)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/DQN_try.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m        Q[state] \u001b[39m=\u001b[39m q_values\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/DQN_try.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m print_by_dict(env,Q)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "Q = {}\n",
    "for y in range(env.size):\n",
    "    for x in range(env.size):\n",
    "       state = (y,x)\n",
    "       q_values = agent.policy_net(state)\n",
    "       Q[state] = q_values\n",
    "print_by_dict(env,Q)\n",
    "                                                                                                                                                                                                                     \n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = {}\n",
    "for state in Q.keys():\n",
    "    V[state] = torch.max(Q[state]).item()\n",
    "print_by_dict(env, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(V)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
