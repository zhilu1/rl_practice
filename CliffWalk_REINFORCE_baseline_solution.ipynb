{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload \n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "%autoreload 2\n",
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter # type: ignore\n",
    "\n",
    "# from agents.A2C import A2CAgent\n",
    "from agents.A2C_cliff import A2CAgent\n",
    "from tools.helper import *\n",
    "import  gymnasium  as gym\n",
    "from rl_envs.new_gym_grid_world_env import GridWorldEnv\n",
    "from torch.nn import functional as F\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 1e-2\n",
    "DISCOUNTED_FACTOR = 0.99\n",
    "\n",
    "FORBIDDEN_REWARD = -10\n",
    "HITWALL_REWARD = -10\n",
    "TARGET_REWARD = 1\n",
    "\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2386fa73290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = GridWorldEnv(size=3,fixed_map = True, seed=SEED, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "\n",
    "# env = gym.make(\"CliffWalking-v0\")\n",
    "# torch.autograd.set_detect_anomaly(False, check_nan=True)\n",
    "# env.seed(args.seed)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_lengths = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 @ Episode 1/300000 (-100.0, 0.0)\n",
      "Step 1000 @ Episode 1/300000 (-9317.0, 0.0)\n",
      "Step 2000 @ Episode 1/300000 (-19128.0, 0.0)\n",
      "Step 3000 @ Episode 1/300000 (-29434.0, 0.0)\n",
      "Step 4000 @ Episode 1/300000 (-40136.0, 0.0)\n",
      "Step 5000 @ Episode 1/300000 (-51927.0, 0.0)\n",
      "Step 6000 @ Episode 1/300000 (-59956.0, 0.0)\n",
      "Step 7000 @ Episode 1/300000 (-69371.0, 0.0)\n",
      "Step 8000 @ Episode 1/300000 (-79677.0, 0.0)\n",
      "Step 9000 @ Episode 1/300000 (-88795.0, 0.0)\n",
      "Step 10000 @ Episode 1/300000 (-99002.0, 0.0)\n",
      "Step 0 @ Episode 2/300000 (-1.0, -107005.0)\n",
      "Step 1000 @ Episode 2/300000 (-11792.0, -107005.0)\n",
      "Step 2000 @ Episode 2/300000 (-20415.0, -107005.0)\n",
      "Step 3000 @ Episode 2/300000 (-33889.0, -107005.0)\n",
      "Step 4000 @ Episode 2/300000 (-45878.0, -107005.0)\n",
      "Step 5000 @ Episode 2/300000 (-59154.0, -107005.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\CliffWalk_REINFORCE_baseline_solution.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m# del agent.saved_log_prob\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(state) \u001b[39m# action 这里也有随机性\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     next_state, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     episode_record\u001b[39m.\u001b[39mappend((state, action, reward, next_state))\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\agents\\A2C_cliff.py:82\u001b[0m, in \u001b[0;36mA2CAgent.get_action\u001b[1;34m(self, in_state, optimal)\u001b[0m\n\u001b[0;32m     79\u001b[0m m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mCategorical(action_probs)\n\u001b[0;32m     80\u001b[0m action \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39msample()\n\u001b[1;32m---> 82\u001b[0m logProb \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39;49mlog_prob(action)\n\u001b[0;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaved_log_prob \u001b[39m=\u001b[39m logProb\n\u001b[0;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaved_log_probs\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, logProb)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\distributions\\categorical.py:127\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    125\u001b[0m value, log_pmf \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits)\n\u001b[0;32m    126\u001b[0m value \u001b[39m=\u001b[39m value[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m log_pmf\u001b[39m.\u001b[39;49mgather(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, value)\u001b[39m.\u001b[39;49msqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "agent = A2CAgent(int(env.observation_space.n), int(env.action_space.n),  discounted_factor=0.99999,  save_action=True)\n",
    "\n",
    "# env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# agent = A2CAgent(2, 5, height= env.height, width=env.width, discounted_factor=DISCOUNTED_FACTOR)\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# agent = A2CAgent(4, 2, lr = LEARN_RATE, discounted_factor=DISCOUNTED_FACTOR)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "num_episodes = 300000\n",
    "episode_len = 10000\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "# 第一次收集改为随机收集\n",
    "trajectory = []\n",
    "obs, _ = env.reset()\n",
    "# for _ in range(1000):\n",
    "#     state = tuple(obs['agent'])\n",
    "#     action = agent.get_behavior_action(state)\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     trajectory.append((state, action, reward+10))\n",
    "running_reward = -10\n",
    "episode_rewards = defaultdict(float)\n",
    "for i_episode  in range(num_episodes):\n",
    "    # 首先, 根据 policy 生成 episode\n",
    "    next_state, _ = env.reset()\n",
    "    # trap_flag = False\n",
    "    # 初始策略是不是有比较大的影响? \n",
    "    episode_record = []\n",
    "    del agent.saved_log_probs[:]\n",
    "    for t in itertools.count():\n",
    "        # del agent.saved_log_prob\n",
    "        state = next_state\n",
    "        action = agent.get_action(state) # action 这里也有随机性\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_record.append((state, action, reward, next_state))\n",
    "        # Update statistics\n",
    "        episode_rewards[i_episode] += reward\n",
    "        if t % 1000 == 0:\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({}, {})\".format(\n",
    "                    t, i_episode + 1, num_episodes, episode_rewards[i_episode], episode_rewards[i_episode - 1]))\n",
    "        if terminated or truncated or reward == -10:\n",
    "            break\n",
    "    discounted_return = 0\n",
    "    for i, (state, action, reward, next_state) in enumerate(reversed(episode_record)):\n",
    "        # calculate TD target\n",
    "        discounted_return = agent.discounted_factor * discounted_return + reward\n",
    "        \n",
    "        v_value = agent.value_net(state)\n",
    "        # value_next = agent.value_net(next_state)\n",
    "\n",
    "        # TD_target = reward + agent.discounted_factor * value_next\n",
    "        advantage = discounted_return - v_value\n",
    "\n",
    "        # updates\n",
    "        agent.optimizer_v.zero_grad()\n",
    "        loss2 =  F.mse_loss(torch.tensor(discounted_return), v_value.squeeze())\n",
    "        loss2.sum().backward(retain_graph=True)\n",
    "        agent.optimizer_v.step()\n",
    "\n",
    "        # action_probs = agent.policy_net(state)        \n",
    "        # action_prob = action_probs[action]\n",
    "        action_prob = agent.saved_log_probs[i]\n",
    "\n",
    "        agent.optimizer.zero_grad()\n",
    "        # loss = - torch.log(action_prob) * advantage\n",
    "        loss = - action_prob * advantage\n",
    "        loss.sum().backward()\n",
    "        # torch.nn.utils.clip_grad.clip_grad_norm_(agent.policy_net.parameters(), 100)\n",
    "        agent.optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# output \n",
    "# \"\"\"\n",
    "# action_mappings = [\"↑\", \"→\", \"↓\", \"←\"]\n",
    "# for i in range(48):\n",
    "#     if i % 12 == 0:\n",
    "#         print()\n",
    "#     action = agent.get_action(i)\n",
    "#     print(action_mappings[action], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00 0.00 0.00 0.00 0.00 ]\n",
      "[ 0.00 -10.00 -10.00 0.00 0.00 ]\n",
      "[ 0.00 0.00 -10.00 0.00 0.00 ]\n",
      "[ 0.00 -10.00 1.00 -10.00 0.00 ]\n",
      "[ 0.00 -10.00 0.00 0.00 0.00 ]\n",
      "\n",
      "[ [[0.05448444 0.7002051  0.00596851 0.00402115 0.23532088]] [[0.00799906 0.81794536 0.00550334 0.02788417 0.14066802]] [[0.0067288  0.7907838  0.00520222 0.04675211 0.15053302]] [[0.5114877  0.22043052 0.00483145 0.04819528 0.21505506]] [[0.7650036  0.011203   0.00682375 0.09995566 0.11701391]] ]\n",
      "[ [[0.1994018  0.00824627 0.6274428  0.00681459 0.15809461]] [[0.0727447  0.0526539  0.5979391  0.22148496 0.05517739]] [[0.03413967 0.7674575  0.15028743 0.01947347 0.02864189]] [[0.3349588  0.4190122  0.04481204 0.00540841 0.19580865]] [[0.73728496 0.00667041 0.05386091 0.08127114 0.12091255]] ]\n",
      "[ [[0.10492554 0.06758191 0.62568295 0.00526727 0.19654228]] [[0.01317008 0.01543762 0.01304096 0.8232991  0.13505217]] [[0.88904977 0.0488671  0.02026117 0.02207359 0.01974836]] [[0.00673974 0.86289847 0.05750281 0.00341097 0.06944805]] [[0.78233266 0.00466431 0.06158398 0.03952961 0.11188941]] ]\n",
      "[ [[0.03036379 0.00869283 0.783397   0.00452628 0.17302012]] [[0.0191469  0.8857416  0.04176357 0.01731104 0.03603685]] [[0.26568195 0.02779223 0.02381632 0.01783364 0.6648759 ]] [[0.14213419 0.07422177 0.07487746 0.66039085 0.04837568]] [[0.9110401  0.00314898 0.02702665 0.00172092 0.05706322]] ]\n",
      "[ [[0.02274531 0.02331833 0.74310267 0.01423666 0.19659704]] [[0.03089739 0.91836655 0.0139704  0.01790346 0.01886227]] [[0.00256019 0.01700942 0.95197827 0.00128133 0.02717075]] [[0.00246956 0.04803516 0.00202537 0.8926994  0.05477049]] [[0.00304042 0.00318211 0.02595338 0.86878985 0.09903422]] ]\n",
      "[  →   →   →   ↓   ↓  ]\n",
      "[  ↑   ↑   →   →   ↓  ]\n",
      "[  ↑   ←   ↓   →   ↓  ]\n",
      "[  ↑   →   ↺   ←   ↓  ]\n",
      "[  ↑   →   ↑   ←   ←  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\codes\\RL_playground\\agents\\A2C.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(sq_inp, dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# # visualize_in_gym(agent, \"CartPole-v1\")\n",
    "policy = agent.generate_policy_table(env.height, env.width)\n",
    "print(env)\n",
    "print_by_dict(env, policy)\n",
    "\n",
    "for i in range(env.height):\n",
    "    print(\"[\", end=\" \")\n",
    "    for j in range(env.width):\n",
    "        state = (i,j)\n",
    "        action = np.argmax(policy[state])\n",
    "        print(env.action_mappings[action], end=\" \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 1, distance: [array([4, 1]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 0]), array([2, 0]), array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 1]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 2]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 3]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 1]), array([2, 0]), array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 3]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 2]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 3]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 2]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 0]), array([2, 0]), array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 2]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 0]), array([2, 0]), array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([4, 1]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 1]), array([2, 0]), array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([4, 0]), array([3, 0]), array([2, 0]), array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 0]), array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 2]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 3]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 3]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 3]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 2]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 0]), array([2, 0]), array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([3, 0]), array([2, 0]), array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([4, 0]), array([3, 0]), array([2, 0]), array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 0]), array([0, 0]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([1, 1]), array([0, 1]), array([0, 2]), array([0, 3]), array([1, 3]), array([1, 4]), array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n",
      "reward: 1, distance: [array([2, 4]), array([3, 4]), array([4, 4]), array([4, 3]), array([4, 2]), array([3, 2])]\n",
      "TERMINATE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\CliffWalk_REINFORCE_baseline_solution.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m gridworld_demo(agent, env, repeat_times\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\tools\\helper.py:175\u001b[0m, in \u001b[0;36mgridworld_demo\u001b[1;34m(agent, env, repeat_times)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(repeat_times):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# obs = tuple(obs)\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(obs, optimal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 175\u001b[0m     obs, reward, terminated, truncated, info  \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    176\u001b[0m     \u001b[39m# VecEnv resets automatically\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\rl_envs\\new_gym_grid_world_env.py:150\u001b[0m, in \u001b[0;36mGridWorldEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    147\u001b[0m info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_info()\n\u001b[0;32m    149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_frame()\n\u001b[0;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_step\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    153\u001b[0m truncated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\rl_envs\\new_gym_grid_world_env.py:256\u001b[0m, in \u001b[0;36mGridWorldEnv._render_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m    254\u001b[0m     \u001b[39m# We need to ensure that human-rendering occurs at the predefined framerate.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m     \u001b[39m# The following line will automatically add a delay to keep the framerate stable.\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# rgb_array\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    259\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(canvas)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    260\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gridworld_demo(agent, env, repeat_times=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\CliffWalk_REINFORCE_baseline_solution.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# env.max_steps = 10\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# gridworld_demo(agent, env, repeat_times=500)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# visualize_in_gym(agent, \"CartPole-v1\")\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/CliffWalk_REINFORCE_baseline_solution.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m visualize_in_gym(agent, \u001b[39m\"\u001b[39;49m\u001b[39mCliffWalking-v0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\tools\\helper.py:116\u001b[0m, in \u001b[0;36mvisualize_in_gym\u001b[1;34m(agent, env_name, inp_env, steps)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[0;32m    115\u001b[0m     observation \u001b[39m=\u001b[39m (observation,)\n\u001b[1;32m--> 116\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(\n\u001b[0;32m    117\u001b[0m         observation\n\u001b[0;32m    118\u001b[0m         , optimal\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    119\u001b[0m     )  \u001b[39m# agent policy that uses the observation and info\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[39m# insert an algorithm that can interact with env and output an action here\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m demo_env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\agents\\A2C.py:96\u001b[0m, in \u001b[0;36mget_action\u001b[1;34m(self, in_state, optimal)\u001b[0m\n\u001b[0;32m     90\u001b[0m def get_action(self, in_state, optimal=False):\n\u001b[0;32m     91\u001b[0m     # with torch.no_grad(): # 哪里都 no_grad 只会害了你 \n\u001b[0;32m     92\u001b[0m     # state = torch.tensor(in_state, dtype=torch.int64)\n\u001b[0;32m     93\u001b[0m     # state = torch.nn.functional.one_hot(state, 48)\n\u001b[0;32m     94\u001b[0m     # with torch.no_grad():\n\u001b[0;32m     95\u001b[0m     action_probs = self.policy_net(in_state)\n\u001b[1;32m---> 96\u001b[0m     # action_probs = (actions_val/actions_val.sum()).detach().numpy()\n\u001b[0;32m     97\u001b[0m     if optimal:\n\u001b[0;32m     98\u001b[0m         return torch.argmax(action_probs).item()\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\agents\\A2C.py:18\u001b[0m, in \u001b[0;36mPolicyNet.forward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inp):\n\u001b[1;32m---> 18\u001b[0m     sq_inp \u001b[39m=\u001b[39m inp[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m+\u001b[39m inp[\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     19\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(sq_inp, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[0;32m     20\u001b[0m     out1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mone_hot(out, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# env.max_steps = 10\n",
    "# gridworld_demo(agent, env, repeat_times=500)\n",
    "# gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# visualize_in_gym(agent, \"CartPole-v1\")\n",
    "# visualize_in_gym(agent, \"CliffWalking-v0\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
