{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload \n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "%autoreload 2\n",
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter # type: ignore\n",
    "\n",
    "from rl_envs.gym_grid_world_env import GridWorldEnv\n",
    "from agents.policy_gradient import PGAgent\n",
    "from tools.helper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 1e-4\n",
    "DISCOUNTED_FACTOR = 0.9\n",
    "\n",
    "FORBIDDEN_REWARD = 0\n",
    "HITWALL_REWARD = 0\n",
    "TARGET_REWARD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_lengths = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\REINFORCE_solution.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# discounted_reward = discounted_reward * agent.discounted_factor + reward\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# agent.q[observation][action] = discounted_reward \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# policy update\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m特别注意: 这里 log π 中的 π(a|s) 是选择 a 的概率, policy network 得输出一个概率, 而不是什么 a 的值\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m当然我们可以用输出的值, 归一化一下作为 action 的概率\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m有一个 变体可能, 在 sample action 时就计算 prob并存储, 然后在 update 时就只是计算 reward 从而计算 loss, 将一个 episode 的loss 都加到一起来一起 backward, 然后更新一次 policy network\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m action_probs \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mpolicy_net(torch\u001b[39m.\u001b[39;49mtensor(state, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# action_probs = actions_val/actions_val.sum()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m agent\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = PGAgent(2, env.action_n, lr = LEARN_RATE, discounted_factor=DISCOUNTED_FACTOR)\n",
    "writer = SummaryWriter()\n",
    "num_episodes = 20\n",
    "episode_len = 3000\n",
    "epochs = 100\n",
    "iter_counter = 0\n",
    "# 第一次收集改为随机收集\n",
    "trajectory = []\n",
    "obs, _ = env.reset()\n",
    "for _ in range(1000):\n",
    "    state = tuple(obs['agent'])\n",
    "    action = agent.get_behavior_action(state)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    trajectory.append((state, action, reward))\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    for epoch in range(epochs):\n",
    "        total_l = 0\n",
    "        for t, (state, action, reward) in enumerate(trajectory):\n",
    "        # for state, action, reward in reversed(trajectory):\n",
    "            discounted_reward = sum(DISCOUNTED_FACTOR**i * t[2] for i, t in enumerate(trajectory[t:]))\n",
    "            # discounted_reward = discounted_reward * agent.discounted_factor + reward\n",
    "            # agent.q[observation][action] = discounted_reward \n",
    "            # policy update\n",
    "            \"\"\"\n",
    "            特别注意: 这里 log π 中的 π(a|s) 是选择 a 的概率, policy network 得输出一个概率, 而不是什么 a 的值\n",
    "            当然我们可以用输出的值, 归一化一下作为 action 的概率\n",
    "\n",
    "            有一个 变体可能, 在 sample action 时就计算 prob并存储, 然后在 update 时就只是计算 reward 从而计算 loss, 将一个 episode 的loss 都加到一起来一起 backward, 然后更新一次 policy network\n",
    "            \"\"\"\n",
    "\n",
    "            action_probs = agent.policy_net(torch.tensor(state, dtype=torch.float))\n",
    "            # action_probs = actions_val/actions_val.sum()\n",
    "            agent.optimizer.zero_grad()\n",
    "            loss = -torch.log(action_probs[action]) * discounted_reward\n",
    "            # [parms.grad for name, parms in agent.policy_net.named_parameters()]\n",
    "            # loss = abs(loss)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad.clip_grad_norm_(agent.policy_net.parameters(), 100)\n",
    "            agent.optimizer.step()\n",
    "            writer.add_scalar('Loss', loss, iter_counter)\n",
    "            iter_counter+=1\n",
    "            total_l += loss\n",
    "        writer.add_scalar('episodeLoss', total_l, episode*epochs + epoch)\n",
    "\n",
    "    # 首先, 根据 policy 生成 episode\n",
    "    obs, _ = env.reset()\n",
    "    trajectory = []\n",
    "    # 初始策略是不是有比较大的影响?\n",
    "    for _ in range(episode_len):\n",
    "        state = tuple(obs['agent'])\n",
    "        action = agent.get_action(state) # action 这里也有随机性\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        trajectory.append((state, action, reward))\n",
    "\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [[0.39561397 0.12332556 0.05199818 0.22307943 0.20598286]] [[0.5060537  0.07135594 0.04576454 0.21237789 0.16444787]] [[0.44437984 0.1300606  0.06464551 0.24568307 0.11523104]] [[0.45169136 0.13823198 0.08859667 0.24091221 0.08056778]] [[0.44508648 0.14166999 0.11594912 0.24101715 0.05627727]] ]\n",
      "[ [[0.41710952 0.28278688 0.02887618 0.15853842 0.11268894]] [[0.5682794  0.14071025 0.03349277 0.1442297  0.11328786]] [[0.47954184 0.13724673 0.03768829 0.2255518  0.11997138]] [[0.36380222 0.17916325 0.0440016  0.33160406 0.08142883]] [[0.33177203 0.17673226 0.05406205 0.38231882 0.05511483]] ]\n",
      "[ [[0.20720705 0.54485005 0.04130831 0.10563801 0.10099661]] [[0.41630587 0.27707475 0.06078083 0.11398135 0.13185729]] [[0.53760934 0.07217819 0.04869492 0.15366492 0.18785273]] [[0.2905602  0.14592473 0.04948127 0.40153116 0.11250268]] [[0.16875313 0.1806551  0.0341997  0.5577638  0.05862822]] ]\n",
      "[ [[0.13277824 0.5957768  0.06302667 0.09551053 0.11290771]] [[0.15818414 0.45173243 0.1574121  0.09228312 0.14038818]] [[0.24427025 0.13107172 0.21299036 0.14715208 0.26451558]] [[0.19209237 0.08631673 0.10430019 0.43777186 0.17951885]] [[0.11689927 0.13433427 0.06398083 0.6039941  0.08079153]] ]\n",
      "[ [[0.0971366  0.59482354 0.09358535 0.08554333 0.12891124]] [[0.09598201 0.41796398 0.24828763 0.0839689  0.15379752]] [[0.07560129 0.14481765 0.50456    0.07799138 0.19702964]] [[0.08443578 0.08803377 0.32080713 0.31275427 0.19396907]] [[0.06171476 0.0745002  0.14245284 0.6064144  0.11491773]] ]\n",
      "[  ↓   ↓   ↓   ↓   ↓  ]\n",
      "[  ↓   ↓   ↓   ↓   ←  ]\n",
      "[  →   ↓   ↓   ←   ←  ]\n",
      "[  →   →   ↺   ←   ←  ]\n",
      "[  →   →   ↑   ↑   ←  ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "policy = agent.generate_policy_table(env.height, env.width)\n",
    "\n",
    "print_by_dict(env, policy)\n",
    "\n",
    "for i in range(env.height):\n",
    "    print(\"[\", end=\" \")\n",
    "    for j in range(env.width):\n",
    "        state = (i,j)\n",
    "        action = np.argmax(policy[state])\n",
    "        print(env.action_mappings[action], end=\" \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
