{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload \n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "%autoreload 2\n",
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter # noqa\n",
    "\n",
    "from rl_envs.gym_grid_world_env import GridWorldEnv\n",
    "from agents.policy_gradient import PGAgent\n",
    "from tools.helper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 0.001\n",
    "DISCOUNTED_FACTOR = 0.9\n",
    "\n",
    "FORBIDDEN_REWARD = -10\n",
    "HITWALL_REWARD = -10\n",
    "TARGET_REWARD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "agent = PGAgent(2, env.action_n, lr = LEARN_RATE, discounted_factor=DISCOUNTED_FACTOR)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_lengths = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "episode_len = 100\n",
    "iter_counter = 0\n",
    "for episode in range(num_episodes):\n",
    "    # 首先, 根据 policy 生成 episode\n",
    "    state = (0,0)\n",
    "    trajectory = []\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(episode_len):\n",
    "        state = tuple(obs['agent'])\n",
    "        action = agent.get_action(state)\n",
    "        obs, reward, terminated , truncated, info = env.step(action)\n",
    "        # next_state = tuple(obs['agent'])\n",
    "        trajectory.append((state, action, reward))\n",
    "        state = obs\n",
    "\n",
    "    # 然后, 根据生成的 episode, 对 Q value 和 policy 进行更新\n",
    "    for _ in range(10):\n",
    "        last_loss = agent.update(trajectory)  \n",
    "    # iter_counter+=1\n",
    "    writer.add_scalar('Loss', last_loss, episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [[0.04699978 0.8079529  0.0485365  0.04845719 0.04805375]] [[0.00263254 0.988766   0.00266724 0.00296175 0.00297244]] [[2.6110813e-05 9.9988234e-01 2.5975356e-05 3.2191856e-05 3.3329899e-05]] [[1.9309658e-07 9.9999905e-01 1.8518118e-07 2.6153560e-07 2.7853201e-07]] [[1.3337742e-09 1.0000000e+00 1.2251686e-09 1.9816628e-09 2.1631956e-09]] ]\n",
      "[ [[0.0083405  0.96237487 0.00936534 0.01016512 0.0097541 ]] [[4.0088277e-04 9.9818844e-01 4.1187386e-04 5.1616534e-04 4.8258348e-04]] [[6.98770737e-06 9.99966502e-01 6.97672021e-06 1.00511925e-05\n",
      "  9.39305755e-06]] [[6.2289494e-08 9.9999952e-01 6.0441948e-08 9.8219672e-08 9.5578208e-08]] [[4.5959511e-10 1.0000000e+00 4.3028878e-10 8.0313584e-10 7.9973317e-10]] ]\n",
      "[ [[3.8316238e-04 9.9809819e-01 4.5520626e-04 5.6154694e-04 5.0185708e-04]] [[1.7937997e-05 9.9990988e-01 1.9425403e-05 2.8188530e-05 2.4647090e-05]] [[5.3177712e-07 9.9999726e-01 5.2927464e-07 9.0034462e-07 7.7707955e-07]] [[8.7590797e-09 1.0000000e+00 8.5347915e-09 1.6744481e-08 1.4376779e-08]] [[9.2487587e-11 1.0000000e+00 8.7096094e-11 1.9522529e-10 1.7227487e-10]] ]\n",
      "[ [[1.1766076e-05 9.9993527e-01 1.4732005e-05 2.1027490e-05 1.7122673e-05]] [[6.3454553e-07 9.9999654e-01 7.2302390e-07 1.2247807e-06 9.8363978e-07]] [[2.1534403e-08 1.0000000e+00 2.2715728e-08 4.5148830e-08 3.6314933e-08]] [[5.7976501e-10 1.0000000e+00 5.6582727e-10 1.3236068e-09 1.0434228e-09]] [[9.7879907e-12 1.0000000e+00 9.3355627e-12 2.4995511e-11 1.9642940e-11]] ]\n",
      "[ [[3.2146824e-07 9.9999797e-01 4.2717505e-07 6.9684449e-07 5.2263209e-07]] [[2.1518391e-08 1.0000000e+00 2.5322528e-08 4.9804584e-08 3.6486703e-08]] [[8.0068246e-10 1.0000000e+00 8.7477392e-10 2.0617354e-09 1.5238457e-09]] [[2.5294057e-11 1.0000000e+00 2.6012543e-11 7.0907551e-11 5.2292327e-11]] [[5.8831249e-13 1.0000000e+00 5.6510054e-13 1.8093016e-12 1.3051337e-12]] ]\n",
      "[  →   →   →   →   →  ]\n",
      "[  →   →   →   →   →  ]\n",
      "[  →   →   →   →   →  ]\n",
      "[  →   →   →   →   →  ]\n",
      "[  →   →   →   →   →  ]\n"
     ]
    }
   ],
   "source": [
    "policy = agent.generate_policy_table(env.height, env.width)\n",
    "\n",
    "print_by_dict(env, policy)\n",
    "\n",
    "for i in range(env.height):\n",
    "    print(\"[\", end=\" \")\n",
    "    for j in range(env.width):\n",
    "        state = (i,j)\n",
    "        action = np.argmax(policy[state])\n",
    "        print(env.action_mappings[action], end=\" \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
