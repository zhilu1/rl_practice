{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload \n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "%autoreload 2\n",
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter # type: ignore\n",
    "\n",
    "from agents.policy_gradient import PGAgent\n",
    "from tools.helper import *\n",
    "import  gymnasium  as gym\n",
    "from rl_envs.new_gym_grid_world_env import GridWorldEnv\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 1e-2\n",
    "DISCOUNTED_FACTOR = 0.99\n",
    "\n",
    "FORBIDDEN_REWARD = -10\n",
    "HITWALL_REWARD = -10\n",
    "TARGET_REWARD = 1\n",
    "\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a22cd6b2b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = GridWorldEnv(size=3,fixed_map = True, seed=SEED, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "\n",
    "# env = gym.make(\"CliffWalking-v0\")\n",
    "# torch.autograd.set_detect_anomaly(False, check_nan=True)\n",
    "# env.seed(args.seed)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_lengths = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: -149.00\tAverage reward: -16.95\tloss: 6.71\n",
      "Episode 10\tLast reward: -50.00\tAverage reward: -30.21\tloss: 1.90\n",
      "Episode 20\tLast reward: -50.00\tAverage reward: -38.15\tloss: 0.20\n",
      "Episode 30\tLast reward: -50.00\tAverage reward: -42.91\tloss: 0.26\n",
      "Episode 40\tLast reward: -50.00\tAverage reward: -45.75\tloss: 0.15\n",
      "Episode 50\tLast reward: -50.00\tAverage reward: -47.46\tloss: -0.38\n",
      "Episode 60\tLast reward: -50.00\tAverage reward: -48.48\tloss: 0.20\n",
      "Episode 70\tLast reward: -50.00\tAverage reward: -56.97\tloss: 0.79\n",
      "Episode 80\tLast reward: -347.00\tAverage reward: -101.37\tloss: 0.13\n",
      "Episode 90\tLast reward: -50.00\tAverage reward: -110.30\tloss: -0.01\n",
      "Episode 100\tLast reward: -50.00\tAverage reward: -122.88\tloss: -0.88\n",
      "Episode 110\tLast reward: -248.00\tAverage reward: -144.56\tloss: 0.34\n",
      "Episode 120\tLast reward: -1436.00\tAverage reward: -378.81\tloss: 0.00\n",
      "Episode 130\tLast reward: -2426.00\tAverage reward: -621.54\tloss: 0.00\n",
      "Episode 140\tLast reward: -248.00\tAverage reward: -1009.63\tloss: -0.00\n",
      "Episode 150\tLast reward: -2921.00\tAverage reward: -1569.22\tloss: -0.00\n",
      "Episode 160\tLast reward: -248.00\tAverage reward: -1499.95\tloss: 0.00\n",
      "Episode 170\tLast reward: -5000.00\tAverage reward: -1685.39\tloss: 0.00\n",
      "Episode 180\tLast reward: -5000.00\tAverage reward: -2638.66\tloss: 0.00\n",
      "Episode 190\tLast reward: -2426.00\tAverage reward: -2949.09\tloss: -0.00\n",
      "Episode 200\tLast reward: -2624.00\tAverage reward: -2675.72\tloss: 0.00\n",
      "Episode 210\tLast reward: -347.00\tAverage reward: -2265.04\tloss: -0.00\n",
      "Episode 220\tLast reward: -4307.00\tAverage reward: -2363.01\tloss: -0.00\n",
      "Episode 230\tLast reward: -5000.00\tAverage reward: -2648.68\tloss: 0.00\n",
      "Episode 240\tLast reward: -5000.00\tAverage reward: -3073.43\tloss: 0.00\n",
      "Episode 250\tLast reward: -149.00\tAverage reward: -3272.80\tloss: -0.00\n",
      "Episode 260\tLast reward: -644.00\tAverage reward: -3044.45\tloss: -0.00\n",
      "Episode 270\tLast reward: -5000.00\tAverage reward: -3570.99\tloss: 0.00\n",
      "Episode 280\tLast reward: -5000.00\tAverage reward: -3847.14\tloss: 0.00\n",
      "Episode 290\tLast reward: -2515.00\tAverage reward: -3315.08\tloss: 0.00\n",
      "Episode 300\tLast reward: -3020.00\tAverage reward: -2850.24\tloss: 0.00\n",
      "Episode 310\tLast reward: -2525.00\tAverage reward: -2583.81\tloss: 0.00\n",
      "Episode 320\tLast reward: -3515.00\tAverage reward: -2511.38\tloss: 0.00\n",
      "Episode 330\tLast reward: -3416.00\tAverage reward: -2620.11\tloss: 0.00\n",
      "Episode 340\tLast reward: -347.00\tAverage reward: -1684.89\tloss: 0.19\n",
      "Episode 350\tLast reward: -248.00\tAverage reward: -1051.12\tloss: -0.00\n",
      "Episode 360\tLast reward: -50.00\tAverage reward: -653.65\tloss: -0.24\n",
      "Episode 370\tLast reward: -50.00\tAverage reward: -411.43\tloss: -0.00\n",
      "Episode 380\tLast reward: -149.00\tAverage reward: -275.18\tloss: -0.01\n",
      "Episode 390\tLast reward: -50.00\tAverage reward: -192.75\tloss: 0.00\n",
      "Episode 400\tLast reward: -149.00\tAverage reward: -151.84\tloss: 0.00\n",
      "Episode 410\tLast reward: -50.00\tAverage reward: -125.82\tloss: 1.16\n",
      "Episode 420\tLast reward: -149.00\tAverage reward: -141.84\tloss: -0.00\n",
      "Episode 430\tLast reward: -149.00\tAverage reward: -124.36\tloss: 0.49\n",
      "Episode 440\tLast reward: -50.00\tAverage reward: -101.81\tloss: 0.62\n",
      "Episode 450\tLast reward: -446.00\tAverage reward: -153.81\tloss: 0.24\n",
      "Episode 460\tLast reward: -50.00\tAverage reward: -137.20\tloss: 0.00\n",
      "Episode 470\tLast reward: -50.00\tAverage reward: -127.60\tloss: -0.02\n",
      "Episode 480\tLast reward: -50.00\tAverage reward: -114.07\tloss: -0.03\n",
      "Episode 490\tLast reward: -50.00\tAverage reward: -114.83\tloss: 0.14\n",
      "Episode 500\tLast reward: -149.00\tAverage reward: -110.96\tloss: 0.06\n",
      "Episode 510\tLast reward: -50.00\tAverage reward: -126.87\tloss: -2.96\n",
      "Episode 520\tLast reward: -50.00\tAverage reward: -125.69\tloss: -0.52\n",
      "Episode 530\tLast reward: -149.00\tAverage reward: -112.04\tloss: 0.00\n",
      "Episode 540\tLast reward: -149.00\tAverage reward: -153.51\tloss: 0.04\n",
      "Episode 550\tLast reward: -248.00\tAverage reward: -162.11\tloss: 0.00\n",
      "Episode 560\tLast reward: -347.00\tAverage reward: -278.29\tloss: -0.00\n",
      "Episode 570\tLast reward: -1733.00\tAverage reward: -511.81\tloss: -0.00\n",
      "Episode 580\tLast reward: -347.00\tAverage reward: -637.88\tloss: 0.00\n",
      "Episode 590\tLast reward: -50.00\tAverage reward: -837.68\tloss: -0.00\n",
      "Episode 600\tLast reward: -941.00\tAverage reward: -800.51\tloss: -0.00\n",
      "Episode 610\tLast reward: -149.00\tAverage reward: -794.58\tloss: 0.00\n",
      "Episode 620\tLast reward: -4703.00\tAverage reward: -1040.24\tloss: 0.00\n",
      "Episode 630\tLast reward: -50.00\tAverage reward: -959.27\tloss: -0.00\n",
      "Episode 640\tLast reward: -644.00\tAverage reward: -860.48\tloss: 0.00\n",
      "Episode 650\tLast reward: -644.00\tAverage reward: -661.06\tloss: 0.00\n",
      "Episode 660\tLast reward: -347.00\tAverage reward: -523.37\tloss: -0.00\n",
      "Episode 670\tLast reward: -941.00\tAverage reward: -599.81\tloss: 0.00\n",
      "Episode 680\tLast reward: -446.00\tAverage reward: -463.11\tloss: 0.06\n",
      "Episode 690\tLast reward: -50.00\tAverage reward: -382.17\tloss: -0.00\n",
      "Episode 700\tLast reward: -3020.00\tAverage reward: -490.94\tloss: -0.00\n",
      "Episode 710\tLast reward: -347.00\tAverage reward: -509.16\tloss: 0.00\n",
      "Episode 720\tLast reward: -248.00\tAverage reward: -432.69\tloss: 0.00\n",
      "Episode 730\tLast reward: -347.00\tAverage reward: -439.89\tloss: 0.00\n",
      "Episode 740\tLast reward: -50.00\tAverage reward: -491.91\tloss: 0.00\n",
      "Episode 750\tLast reward: -50.00\tAverage reward: -422.36\tloss: 0.00\n",
      "Episode 760\tLast reward: -545.00\tAverage reward: -701.30\tloss: -0.00\n",
      "Episode 770\tLast reward: -37.00\tAverage reward: -627.16\tloss: -0.10\n",
      "Episode 780\tLast reward: -2129.00\tAverage reward: -728.09\tloss: 0.00\n",
      "Episode 790\tLast reward: -1040.00\tAverage reward: -950.72\tloss: 0.01\n",
      "Episode 800\tLast reward: -1238.00\tAverage reward: -1173.12\tloss: -0.00\n",
      "Episode 810\tLast reward: -1139.00\tAverage reward: -1366.61\tloss: 0.64\n",
      "Episode 820\tLast reward: -1634.00\tAverage reward: -1594.09\tloss: -0.00\n",
      "Episode 830\tLast reward: -2228.00\tAverage reward: -1574.42\tloss: 0.00\n",
      "Episode 840\tLast reward: -1634.00\tAverage reward: -1564.36\tloss: 0.00\n",
      "Episode 850\tLast reward: -1139.00\tAverage reward: -1530.83\tloss: 0.00\n",
      "Episode 860\tLast reward: -2327.00\tAverage reward: -1566.07\tloss: -0.00\n",
      "Episode 870\tLast reward: -1436.00\tAverage reward: -1495.86\tloss: 0.00\n",
      "Episode 880\tLast reward: -1337.00\tAverage reward: -1505.15\tloss: 0.00\n",
      "Episode 890\tLast reward: -1535.00\tAverage reward: -1482.00\tloss: -0.00\n",
      "Episode 900\tLast reward: -1238.00\tAverage reward: -1520.14\tloss: 0.00\n",
      "Episode 910\tLast reward: -1931.00\tAverage reward: -1484.98\tloss: 0.00\n",
      "Episode 920\tLast reward: -1931.00\tAverage reward: -1487.92\tloss: 0.00\n",
      "Episode 930\tLast reward: -1337.00\tAverage reward: -1467.80\tloss: 0.00\n",
      "Episode 940\tLast reward: -1634.00\tAverage reward: -1434.21\tloss: -0.00\n",
      "Episode 950\tLast reward: -1238.00\tAverage reward: -1411.89\tloss: -0.00\n",
      "Episode 960\tLast reward: -1733.00\tAverage reward: -1459.26\tloss: 0.00\n",
      "Episode 970\tLast reward: -1931.00\tAverage reward: -1528.87\tloss: -0.00\n",
      "Episode 980\tLast reward: -1634.00\tAverage reward: -1540.82\tloss: 0.00\n",
      "Episode 990\tLast reward: -2030.00\tAverage reward: -1609.54\tloss: -0.00\n",
      "Episode 1000\tLast reward: -1238.00\tAverage reward: -1529.71\tloss: -0.00\n",
      "Episode 1010\tLast reward: -1040.00\tAverage reward: -1535.11\tloss: 0.00\n",
      "Episode 1020\tLast reward: -2129.00\tAverage reward: -1593.87\tloss: 0.00\n",
      "Episode 1030\tLast reward: -1436.00\tAverage reward: -1564.23\tloss: 0.00\n",
      "Episode 1040\tLast reward: -1634.00\tAverage reward: -1574.41\tloss: 0.00\n",
      "Episode 1050\tLast reward: -1139.00\tAverage reward: -1573.59\tloss: 0.00\n",
      "Episode 1060\tLast reward: -1535.00\tAverage reward: -1560.40\tloss: 0.00\n",
      "Episode 1070\tLast reward: -1832.00\tAverage reward: -1589.45\tloss: 0.00\n",
      "Episode 1080\tLast reward: -2030.00\tAverage reward: -1642.12\tloss: 0.00\n",
      "Episode 1090\tLast reward: -1733.00\tAverage reward: -1678.05\tloss: 0.00\n",
      "Episode 1100\tLast reward: -2030.00\tAverage reward: -1691.82\tloss: 0.19\n",
      "Episode 1110\tLast reward: -1238.00\tAverage reward: -1625.94\tloss: 0.00\n",
      "Episode 1120\tLast reward: -1238.00\tAverage reward: -1523.44\tloss: 0.00\n",
      "Episode 1130\tLast reward: -941.00\tAverage reward: -1438.60\tloss: 0.00\n",
      "Episode 1140\tLast reward: -1139.00\tAverage reward: -1415.51\tloss: 0.00\n",
      "Episode 1150\tLast reward: -1535.00\tAverage reward: -1467.16\tloss: 0.00\n",
      "Episode 1160\tLast reward: -941.00\tAverage reward: -1462.98\tloss: 0.00\n",
      "Episode 1170\tLast reward: -1733.00\tAverage reward: -1426.04\tloss: 0.00\n",
      "Episode 1180\tLast reward: -1535.00\tAverage reward: -1425.52\tloss: 0.00\n",
      "Episode 1190\tLast reward: -1040.00\tAverage reward: -1452.76\tloss: 0.00\n",
      "Episode 1200\tLast reward: -1238.00\tAverage reward: -1406.33\tloss: 0.00\n",
      "Episode 1210\tLast reward: -1139.00\tAverage reward: -1397.74\tloss: 0.00\n",
      "Episode 1220\tLast reward: -644.00\tAverage reward: -1352.91\tloss: 0.00\n",
      "Episode 1230\tLast reward: -743.00\tAverage reward: -1266.69\tloss: -0.00\n",
      "Episode 1240\tLast reward: -941.00\tAverage reward: -1170.68\tloss: 0.00\n",
      "Episode 1250\tLast reward: -1436.00\tAverage reward: -1117.78\tloss: -0.00\n",
      "Episode 1260\tLast reward: -842.00\tAverage reward: -1140.26\tloss: 0.00\n",
      "Episode 1270\tLast reward: -1337.00\tAverage reward: -1082.86\tloss: 0.00\n",
      "Episode 1280\tLast reward: -743.00\tAverage reward: -1074.61\tloss: -0.00\n",
      "Episode 1290\tLast reward: -842.00\tAverage reward: -979.52\tloss: 0.00\n",
      "Episode 1300\tLast reward: -644.00\tAverage reward: -890.98\tloss: 0.00\n",
      "Episode 1310\tLast reward: -446.00\tAverage reward: -768.12\tloss: 0.00\n",
      "Episode 1320\tLast reward: -545.00\tAverage reward: -726.47\tloss: 0.00\n",
      "Episode 1330\tLast reward: -644.00\tAverage reward: -682.39\tloss: 0.00\n",
      "Episode 1340\tLast reward: -545.00\tAverage reward: -646.20\tloss: 0.00\n",
      "Episode 1350\tLast reward: -545.00\tAverage reward: -694.40\tloss: -0.00\n",
      "Episode 1360\tLast reward: -842.00\tAverage reward: -656.97\tloss: 0.00\n",
      "Episode 1370\tLast reward: -842.00\tAverage reward: -681.79\tloss: 0.00\n",
      "Episode 1380\tLast reward: -941.00\tAverage reward: -737.99\tloss: 0.00\n",
      "Episode 1390\tLast reward: -446.00\tAverage reward: -739.00\tloss: 0.00\n",
      "Episode 1400\tLast reward: -545.00\tAverage reward: -674.91\tloss: 0.00\n",
      "Episode 1410\tLast reward: -743.00\tAverage reward: -674.68\tloss: 0.00\n",
      "Episode 1420\tLast reward: -743.00\tAverage reward: -696.36\tloss: -0.15\n",
      "Episode 1430\tLast reward: -446.00\tAverage reward: -733.99\tloss: 0.00\n",
      "Episode 1440\tLast reward: -743.00\tAverage reward: -690.09\tloss: -1.26\n",
      "Episode 1450\tLast reward: -545.00\tAverage reward: -649.35\tloss: 0.00\n",
      "Episode 1460\tLast reward: -545.00\tAverage reward: -661.74\tloss: 0.00\n",
      "Episode 1470\tLast reward: -644.00\tAverage reward: -687.42\tloss: 0.00\n",
      "Episode 1480\tLast reward: -545.00\tAverage reward: -657.07\tloss: 0.00\n",
      "Episode 1490\tLast reward: -545.00\tAverage reward: -708.05\tloss: 0.00\n",
      "Episode 1500\tLast reward: -446.00\tAverage reward: -682.95\tloss: 0.00\n",
      "Episode 1510\tLast reward: -446.00\tAverage reward: -673.46\tloss: 0.00\n",
      "Episode 1520\tLast reward: -1040.00\tAverage reward: -656.77\tloss: 0.00\n",
      "Episode 1530\tLast reward: -842.00\tAverage reward: -683.60\tloss: 0.00\n",
      "Episode 1540\tLast reward: -545.00\tAverage reward: -663.91\tloss: 0.00\n",
      "Episode 1550\tLast reward: -1238.00\tAverage reward: -681.59\tloss: 0.00\n",
      "Episode 1560\tLast reward: -545.00\tAverage reward: -691.81\tloss: 0.00\n",
      "Episode 1570\tLast reward: -446.00\tAverage reward: -670.50\tloss: 0.00\n",
      "Episode 1580\tLast reward: -941.00\tAverage reward: -731.45\tloss: 0.00\n",
      "Episode 1590\tLast reward: -1040.00\tAverage reward: -854.93\tloss: -0.00\n",
      "Episode 1600\tLast reward: -1238.00\tAverage reward: -944.51\tloss: 0.00\n",
      "Episode 1610\tLast reward: -1337.00\tAverage reward: -988.96\tloss: 0.00\n",
      "Episode 1620\tLast reward: -1040.00\tAverage reward: -986.89\tloss: 0.00\n",
      "Episode 1630\tLast reward: -1238.00\tAverage reward: -1014.48\tloss: 0.00\n",
      "Episode 1640\tLast reward: -1139.00\tAverage reward: -1064.19\tloss: -0.00\n",
      "Episode 1650\tLast reward: -1139.00\tAverage reward: -1117.25\tloss: 0.00\n",
      "Episode 1660\tLast reward: -1337.00\tAverage reward: -1199.43\tloss: 0.00\n",
      "Episode 1670\tLast reward: -842.00\tAverage reward: -1137.89\tloss: 0.00\n",
      "Episode 1680\tLast reward: -743.00\tAverage reward: -1119.22\tloss: 0.00\n",
      "Episode 1690\tLast reward: -842.00\tAverage reward: -1147.03\tloss: 0.00\n",
      "Episode 1700\tLast reward: -1436.00\tAverage reward: -1111.87\tloss: 0.00\n",
      "Episode 1710\tLast reward: -941.00\tAverage reward: -1087.85\tloss: 0.00\n",
      "Episode 1720\tLast reward: -743.00\tAverage reward: -1043.25\tloss: -0.53\n",
      "Episode 1730\tLast reward: -842.00\tAverage reward: -1053.19\tloss: 0.00\n",
      "Episode 1740\tLast reward: -1040.00\tAverage reward: -1050.73\tloss: 0.00\n",
      "Episode 1750\tLast reward: -1337.00\tAverage reward: -1080.68\tloss: 0.00\n",
      "Episode 1760\tLast reward: -1634.00\tAverage reward: -1155.34\tloss: 0.00\n",
      "Episode 1770\tLast reward: -743.00\tAverage reward: -1117.95\tloss: 0.00\n",
      "Episode 1780\tLast reward: -1436.00\tAverage reward: -1076.51\tloss: -0.00\n",
      "Episode 1790\tLast reward: -1040.00\tAverage reward: -1038.46\tloss: -0.00\n",
      "Episode 1800\tLast reward: -1535.00\tAverage reward: -1077.41\tloss: 0.00\n",
      "Episode 1810\tLast reward: -1337.00\tAverage reward: -1074.82\tloss: 0.00\n",
      "Episode 1820\tLast reward: -1238.00\tAverage reward: -1091.81\tloss: 0.00\n",
      "Episode 1830\tLast reward: -1040.00\tAverage reward: -1070.21\tloss: -0.00\n",
      "Episode 1840\tLast reward: -743.00\tAverage reward: -1091.35\tloss: 0.00\n",
      "Episode 1850\tLast reward: -1733.00\tAverage reward: -1144.43\tloss: -0.00\n",
      "Episode 1860\tLast reward: -1436.00\tAverage reward: -1157.55\tloss: 0.00\n",
      "Episode 1870\tLast reward: -1436.00\tAverage reward: -1259.07\tloss: 0.00\n",
      "Episode 1880\tLast reward: -1040.00\tAverage reward: -1244.59\tloss: 0.00\n",
      "Episode 1890\tLast reward: -1733.00\tAverage reward: -1303.01\tloss: 0.00\n",
      "Episode 1900\tLast reward: -1535.00\tAverage reward: -1343.31\tloss: 0.23\n",
      "Episode 1910\tLast reward: -1238.00\tAverage reward: -1358.85\tloss: -0.00\n",
      "Episode 1920\tLast reward: -1436.00\tAverage reward: -1347.81\tloss: 0.00\n",
      "Episode 1930\tLast reward: -941.00\tAverage reward: -1357.31\tloss: 0.00\n",
      "Episode 1940\tLast reward: -1337.00\tAverage reward: -1395.08\tloss: 0.00\n",
      "Episode 1950\tLast reward: -1337.00\tAverage reward: -1386.19\tloss: 0.00\n",
      "Episode 1960\tLast reward: -1436.00\tAverage reward: -1388.44\tloss: 0.00\n",
      "Episode 1970\tLast reward: -1535.00\tAverage reward: -1382.13\tloss: 0.00\n",
      "Episode 1980\tLast reward: -1931.00\tAverage reward: -1401.39\tloss: 0.29\n",
      "Episode 1990\tLast reward: -1733.00\tAverage reward: -1468.86\tloss: 0.00\n",
      "Episode 2000\tLast reward: -1436.00\tAverage reward: -1422.71\tloss: 0.00\n",
      "Episode 2010\tLast reward: -1832.00\tAverage reward: -1427.92\tloss: 0.60\n",
      "Episode 2020\tLast reward: -1733.00\tAverage reward: -1419.53\tloss: 0.00\n",
      "Episode 2030\tLast reward: -1337.00\tAverage reward: -1561.77\tloss: -0.35\n",
      "Episode 2040\tLast reward: -2525.00\tAverage reward: -1897.23\tloss: 0.00\n",
      "Episode 2050\tLast reward: -2822.00\tAverage reward: -2138.69\tloss: 0.00\n",
      "Episode 2060\tLast reward: -2129.00\tAverage reward: -2256.74\tloss: 0.00\n",
      "Episode 2070\tLast reward: -2426.00\tAverage reward: -2397.46\tloss: 0.00\n",
      "Episode 2080\tLast reward: -2822.00\tAverage reward: -2486.47\tloss: 0.00\n",
      "Episode 2090\tLast reward: -2327.00\tAverage reward: -2474.85\tloss: 1.26\n",
      "Episode 2100\tLast reward: -3020.00\tAverage reward: -2491.32\tloss: 0.00\n",
      "Episode 2110\tLast reward: -2723.00\tAverage reward: -2494.86\tloss: -0.00\n",
      "Episode 2120\tLast reward: -3020.00\tAverage reward: -2504.97\tloss: 0.00\n",
      "Episode 2130\tLast reward: -2030.00\tAverage reward: -2472.42\tloss: -0.00\n",
      "Episode 2140\tLast reward: -2624.00\tAverage reward: -2490.41\tloss: -0.00\n",
      "Episode 2150\tLast reward: -2723.00\tAverage reward: -2491.36\tloss: 0.00\n",
      "Episode 2160\tLast reward: -2525.00\tAverage reward: -2529.61\tloss: 0.00\n",
      "Episode 2170\tLast reward: -2723.00\tAverage reward: -2631.22\tloss: 0.00\n",
      "Episode 2180\tLast reward: -2723.00\tAverage reward: -2584.83\tloss: 0.00\n",
      "Episode 2190\tLast reward: -2426.00\tAverage reward: -2558.14\tloss: 0.00\n",
      "Episode 2200\tLast reward: -2426.00\tAverage reward: -2627.06\tloss: 0.00\n",
      "Episode 2210\tLast reward: -3020.00\tAverage reward: -2706.11\tloss: 0.00\n",
      "Episode 2220\tLast reward: -2030.00\tAverage reward: -2610.57\tloss: 0.00\n",
      "Episode 2230\tLast reward: -2327.00\tAverage reward: -2580.48\tloss: 0.00\n",
      "Episode 2240\tLast reward: -3020.00\tAverage reward: -2572.94\tloss: -0.00\n",
      "Episode 2250\tLast reward: -3020.00\tAverage reward: -2649.13\tloss: 0.00\n",
      "Episode 2260\tLast reward: -3020.00\tAverage reward: -2628.62\tloss: 0.00\n",
      "Episode 2270\tLast reward: -2030.00\tAverage reward: -2609.54\tloss: 0.00\n",
      "Episode 2280\tLast reward: -2129.00\tAverage reward: -2594.52\tloss: 0.00\n",
      "Episode 2290\tLast reward: -2525.00\tAverage reward: -2576.78\tloss: 0.00\n",
      "Episode 2300\tLast reward: -1832.00\tAverage reward: -2551.16\tloss: 0.00\n",
      "Episode 2310\tLast reward: -2624.00\tAverage reward: -2544.76\tloss: 0.00\n",
      "Episode 2320\tLast reward: -3317.00\tAverage reward: -2585.61\tloss: 0.00\n",
      "Episode 2330\tLast reward: -2426.00\tAverage reward: -2509.60\tloss: 0.00\n",
      "Episode 2340\tLast reward: -2723.00\tAverage reward: -2510.78\tloss: 0.00\n",
      "Episode 2350\tLast reward: -2525.00\tAverage reward: -2510.42\tloss: 0.55\n",
      "Episode 2360\tLast reward: -2921.00\tAverage reward: -2526.06\tloss: 0.00\n",
      "Episode 2370\tLast reward: -2129.00\tAverage reward: -2497.16\tloss: 0.00\n",
      "Episode 2380\tLast reward: -2723.00\tAverage reward: -2527.27\tloss: 0.00\n",
      "Episode 2390\tLast reward: -2921.00\tAverage reward: -2586.61\tloss: 0.00\n",
      "Episode 2400\tLast reward: -3119.00\tAverage reward: -2605.85\tloss: 0.00\n",
      "Episode 2410\tLast reward: -2525.00\tAverage reward: -2628.59\tloss: 0.00\n",
      "Episode 2420\tLast reward: -2921.00\tAverage reward: -2584.66\tloss: 0.00\n",
      "Episode 2430\tLast reward: -2426.00\tAverage reward: -2558.99\tloss: 0.00\n",
      "Episode 2440\tLast reward: -2822.00\tAverage reward: -2585.57\tloss: 0.00\n",
      "Episode 2450\tLast reward: -2228.00\tAverage reward: -2614.39\tloss: 0.00\n",
      "Episode 2460\tLast reward: -2624.00\tAverage reward: -2589.73\tloss: 0.00\n",
      "Episode 2470\tLast reward: -2723.00\tAverage reward: -2670.30\tloss: 0.00\n",
      "Episode 2480\tLast reward: -2327.00\tAverage reward: -2661.65\tloss: 0.00\n",
      "Episode 2490\tLast reward: -2426.00\tAverage reward: -2579.06\tloss: 0.00\n",
      "Episode 2500\tLast reward: -2525.00\tAverage reward: -2569.74\tloss: 0.00\n",
      "Episode 2510\tLast reward: -3416.00\tAverage reward: -2581.90\tloss: 0.00\n",
      "Episode 2520\tLast reward: -2822.00\tAverage reward: -2595.76\tloss: -0.37\n",
      "Episode 2530\tLast reward: -2723.00\tAverage reward: -2531.89\tloss: 0.00\n",
      "Episode 2540\tLast reward: -2228.00\tAverage reward: -2538.73\tloss: 0.00\n",
      "Episode 2550\tLast reward: -2822.00\tAverage reward: -2464.40\tloss: 0.00\n",
      "Episode 2560\tLast reward: -2426.00\tAverage reward: -2556.54\tloss: 0.00\n",
      "Episode 2570\tLast reward: -2723.00\tAverage reward: -2521.57\tloss: -0.00\n",
      "Episode 2580\tLast reward: -2822.00\tAverage reward: -2514.21\tloss: 0.00\n",
      "Episode 2590\tLast reward: -2525.00\tAverage reward: -2474.88\tloss: 0.00\n",
      "Episode 2600\tLast reward: -3317.00\tAverage reward: -2445.92\tloss: 0.00\n",
      "Episode 2610\tLast reward: -2327.00\tAverage reward: -2402.93\tloss: 0.00\n",
      "Episode 2620\tLast reward: -2921.00\tAverage reward: -2464.97\tloss: 0.00\n",
      "Episode 2630\tLast reward: -2525.00\tAverage reward: -2483.33\tloss: 0.00\n",
      "Episode 2640\tLast reward: -2723.00\tAverage reward: -2535.73\tloss: 0.00\n",
      "Episode 2650\tLast reward: -2426.00\tAverage reward: -2567.33\tloss: 0.00\n",
      "Episode 2660\tLast reward: -2525.00\tAverage reward: -2522.90\tloss: 0.00\n",
      "Episode 2670\tLast reward: -2327.00\tAverage reward: -2539.05\tloss: 0.00\n",
      "Episode 2680\tLast reward: -2228.00\tAverage reward: -2542.47\tloss: 0.00\n",
      "Episode 2690\tLast reward: -2129.00\tAverage reward: -2525.84\tloss: 0.00\n",
      "Episode 2700\tLast reward: -2129.00\tAverage reward: -2478.49\tloss: 0.00\n",
      "Episode 2710\tLast reward: -2228.00\tAverage reward: -2463.00\tloss: 0.00\n",
      "Episode 2720\tLast reward: -2525.00\tAverage reward: -2476.33\tloss: -0.00\n",
      "Episode 2730\tLast reward: -2426.00\tAverage reward: -2418.55\tloss: -0.00\n",
      "Episode 2740\tLast reward: -2921.00\tAverage reward: -2453.05\tloss: 0.00\n",
      "Episode 2750\tLast reward: -2426.00\tAverage reward: -2483.38\tloss: 0.00\n",
      "Episode 2760\tLast reward: -2426.00\tAverage reward: -2522.93\tloss: 0.00\n",
      "Episode 2770\tLast reward: -2426.00\tAverage reward: -2593.81\tloss: 0.00\n",
      "Episode 2780\tLast reward: -2723.00\tAverage reward: -2556.29\tloss: 0.00\n",
      "Episode 2790\tLast reward: -2624.00\tAverage reward: -2587.46\tloss: 0.00\n",
      "Episode 2800\tLast reward: -2723.00\tAverage reward: -2519.44\tloss: 0.00\n",
      "Episode 2810\tLast reward: -2921.00\tAverage reward: -2562.06\tloss: 0.00\n",
      "Episode 2820\tLast reward: -2921.00\tAverage reward: -2541.87\tloss: 0.00\n",
      "Episode 2830\tLast reward: -2327.00\tAverage reward: -2531.09\tloss: 0.00\n",
      "Episode 2840\tLast reward: -2624.00\tAverage reward: -2527.50\tloss: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\REINFORCE_solution.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m real_episode_len \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episode_len):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# state = obs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     state \u001b[39m=\u001b[39m (obs,)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(state) \u001b[39m# action 这里也有随机性\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     sa_pair \u001b[39m=\u001b[39m (state, action)\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\agents\\policy_gradient.py:63\u001b[0m, in \u001b[0;36mPGAgent.get_action\u001b[1;34m(self, in_state, optimal)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action\u001b[39m(\u001b[39mself\u001b[39m, in_state, optimal\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     61\u001b[0m     \u001b[39m# with torch.no_grad(): # 哪里都 no_grad 只会害了你 \u001b[39;00m\n\u001b[0;32m     62\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(in_state, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m     action_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net(state)\n\u001b[0;32m     64\u001b[0m     \u001b[39m# action_probs = (actions_val/actions_val.sum()).detach().numpy()\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m optimal:\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"CliffWalking-v0\")\n",
    "# agent = PGAgent(1, int(env.action_space.n), lr = LEARN_RATE, discounted_factor=0.99)\n",
    "\n",
    "# env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# agent = PGAgent(2, 5, lr = LEARN_RATE, discounted_factor=DISCOUNTED_FACTOR)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = PGAgent(4, 2, lr = LEARN_RATE, discounted_factor=DISCOUNTED_FACTOR)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "num_episodes = 200000\n",
    "episode_len = 50\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "# 第一次收集改为随机收集\n",
    "trajectory = []\n",
    "obs, _ = env.reset()\n",
    "# for _ in range(1000):\n",
    "#     state = tuple(obs['agent'])\n",
    "#     action = agent.get_behavior_action(state)\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     trajectory.append((state, action, reward+10))\n",
    "running_reward = -10\n",
    "for episode in range(num_episodes):\n",
    "    # 首先, 根据 policy 生成 episode\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0.\n",
    "    real_episode_len = 0\n",
    "    rewards = []\n",
    "    agent.first_occ_set.clear()\n",
    "    del agent.saved_log_probs[:]\n",
    "    # trap_flag = False\n",
    "    # 初始策略是不是有比较大的影响? \n",
    "    for real_episode_len in range(episode_len):\n",
    "        # state = obs\n",
    "        state = (obs,)\n",
    "        action = agent.get_action(state) # action 这里也有随机性\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        sa_pair = (state, action)\n",
    "        if sa_pair not in agent.first_occ_set:\n",
    "            agent.first_occ_set.add(sa_pair)\n",
    "            rewards.append(reward)\n",
    "        # trajectory.append((state, action, reward))\n",
    "        ep_reward += float(reward)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # 试试把所有失败案例都不管, 只考虑成功 (有点像比较极端的欠采样)\n",
    "    # if trap_flag:\n",
    "        # continue\n",
    "\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    policy_loss = []\n",
    "    discounted_reward = 0\n",
    "    returns = []\n",
    "    for t, reward in enumerate(rewards[::-1]):\n",
    "        discounted_reward = discounted_reward * agent.discounted_factor + reward\n",
    "        returns.insert(0, discounted_reward)\n",
    "        # discounted_reward = sum(DISCOUNTED_FACTOR**i * t[2] for i, t in enumerate(trajectory[t:]))\n",
    "    toh_returns = torch.tensor(returns)\n",
    "    std = toh_returns.std() if toh_returns.size(0) != 1 else 0\n",
    "    toh_returns = (toh_returns- toh_returns.mean()) / (std+ eps)\n",
    "    for log_prob, R in zip(agent.saved_log_probs, toh_returns):\n",
    "        # if R > 0 and episode < num_episodes/2:\n",
    "        #     R *= 10\n",
    "        # else:\n",
    "        #     R = R * R\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    # policy update\n",
    "    \"\"\"\n",
    "    特别注意: 这里 log π 中的 π(a|s) 是选择 a 的概率, policy network 得输出一个概率, 而不是什么 a 的值\n",
    "    当然我们可以用输出的值, 归一化一下作为 action 的概率\n",
    "    \"\"\"\n",
    "    # action_probs = agent.policy_net(torch.tensor(state, dtype=torch.float))\n",
    "    # agent.q[state][action] = discounted_reward \n",
    "    # agent.v[state] = sum([agent.q[state][a] * action_probs[a] for a in agent.q[state].keys()])\n",
    "    # agent.v[state] = np.mean(agent.q[state])\n",
    "\n",
    "    # action_probs = actions_val/actions_val.sum()\n",
    "    agent.optimizer.zero_grad()\n",
    "    \"\"\"\n",
    "    当 discounted reward < 0 时, loss < 0. 若是 action 选择错误, 则 discounted_reward 小, 使得 loss 小 (或者说负地厉害) \n",
    "    梯度下降会将 loss 减地更小, 也就使得对应错误 action 的 action_probs[action] 减小\n",
    "\n",
    "    相反, 当选择正确 action 时, discounted_reward 理想下应该更大, 则 loss 也更大, 梯度下降同样降低 loss,\n",
    "    使得对应正确 action 的 action_probs[action] 减小. \n",
    "    \n",
    "    关键就在于, 要使得 loss 小的时候梯度下降地比 loss 大的时候要更快.\n",
    "\n",
    "    (若是训练地成功 下一轮时 discounted_reward 就会变大, 那么 loss 也就是越来越大, 自然就是向上走,\n",
    "    至于为什么 loss 会趋近于 0, 我猜测是因为 discounted_reward 有一个由负变正的过程, 而在其中当 loss 变为 0 时\n",
    "    ) \n",
    "    \"\"\"\n",
    "    loss = torch.cat(policy_loss).sum()\n",
    "    # loss = -torch.log(action_probs[action]) * (discounted_reward)\n",
    "    # loss = -torch.log(action_probs[action]) * (discounted_reward - agent.v[state]) # add baselline advantage\n",
    "    # [parms.grad for name, parms in agent.policy_net.named_parameters()]\n",
    "    # loss = abs(loss)\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad.clip_grad_norm_(agent.policy_net.parameters(), 100)\n",
    "    agent.optimizer.step()\n",
    "\n",
    "\n",
    "    writer.add_scalar('Loss', loss, episode)\n",
    "    writer.add_scalar('episodeReward', discounted_reward, episode)\n",
    "    writer.add_scalar('ep_reward', ep_reward, episode)\n",
    "    writer.add_scalar('running_reward', running_reward, episode)\n",
    "\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}\\tloss: {:.2f}'.format(\n",
    "                episode, ep_reward, running_reward, loss))\n",
    "    if running_reward > 0.9:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                \"the last episode runs to {} time steps!\".format(running_reward, real_episode_len))\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [[0. 0. 0. 1.]] [[1. 0. 0. 0.]] [[1. 0. 0. 0.]] ]\n",
      "[ [[1. 0. 0. 0.]] [[1. 0. 0. 0.]] [[1. 0. 0. 0.]] ]\n",
      "[ [[1. 0. 0. 0.]] [[1. 0. 0. 0.]] [[1. 0. 0. 0.]] ]\n",
      "[  ←   ↓   ↓  ]\n",
      "[  ↓   ↓   ↓  ]\n",
      "[  ↓   ↓   ↓  ]\n"
     ]
    }
   ],
   "source": [
    "# # visualize_in_gym(agent, \"CartPole-v1\")\n",
    "# policy = agent.generate_policy_table(env.height, env.width)\n",
    "\n",
    "# print_by_dict(env, policy)\n",
    "\n",
    "# for i in range(env.height):\n",
    "#     print(\"[\", end=\" \")\n",
    "#     for j in range(env.width):\n",
    "#         state = (i,j)\n",
    "#         action = np.argmax(policy[state])\n",
    "#         print(env.action_mappings[action], end=\" \")\n",
    "#     print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\codes\\RL_playground\\REINFORCE_solution.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# env.max_steps = 10\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# gridworld_demo(agent, env, repeat_times=500)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# visualize_in_gym(agent, \"CartPole-v1\")\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/codes/RL_playground/REINFORCE_solution.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m visualize_in_gym(agent, \u001b[39m\"\u001b[39;49m\u001b[39mCliffWalking-v0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\codes\\RL_playground\\tools\\helper.py:120\u001b[0m, in \u001b[0;36mvisualize_in_gym\u001b[1;34m(agent, env_name, inp_env, steps)\u001b[0m\n\u001b[0;32m    116\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(\n\u001b[0;32m    117\u001b[0m     observation, optimal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    118\u001b[0m )  \u001b[39m# agent policy that uses the observation and info\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39m# insert an algorithm that can interact with env and output an action here\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m demo_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    121\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n\u001b[0;32m    122\u001b[0m     observation, info \u001b[39m=\u001b[39m demo_env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:181\u001b[0m, in \u001b[0;36mCliffWalkingEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[0;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    182\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:206\u001b[0m, in \u001b[0;36mCliffWalkingEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[0;32m    205\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\finRL_310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:293\u001b[0m, in \u001b[0;36mCliffWalkingEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    291\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m    292\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    294\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# rgb_array\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    296\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    297\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# env.max_steps = 10\n",
    "# gridworld_demo(agent, env, repeat_times=500)\n",
    "# gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# visualize_in_gym(agent, \"CartPole-v1\")\n",
    "visualize_in_gym(agent, \"CliffWalking-v0\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
