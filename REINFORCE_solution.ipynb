{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload \n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "%autoreload 2\n",
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter # type: ignore\n",
    "\n",
    "from rl_envs.gym_grid_world_env import GridWorldEnv\n",
    "from agents.policy_gradient import PGAgent\n",
    "from tools.helper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 1e-4\n",
    "DISCOUNTED_FACTOR = 0.9\n",
    "\n",
    "FORBIDDEN_REWARD = -1\n",
    "HITWALL_REWARD = -1\n",
    "TARGET_REWARD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv(size=3,fixed_map = True, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "# env = GridWorldEnv(fixed_map = True, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_lengths = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: -6.00\tAverage reward: -9.80\n",
      "Episode 100\tLast reward: -14.00\tAverage reward: -6.77\n",
      "Episode 200\tLast reward: 0.00\tAverage reward: -7.14\n",
      "Episode 300\tLast reward: -1.00\tAverage reward: -4.57\n",
      "Episode 400\tLast reward: -1.00\tAverage reward: -6.95\n",
      "Episode 500\tLast reward: -3.00\tAverage reward: -5.83\n",
      "Episode 600\tLast reward: -3.00\tAverage reward: -7.40\n",
      "Episode 700\tLast reward: -7.00\tAverage reward: -6.24\n",
      "Episode 800\tLast reward: -5.00\tAverage reward: -4.97\n",
      "Episode 900\tLast reward: -8.00\tAverage reward: -5.16\n",
      "Episode 1000\tLast reward: 0.00\tAverage reward: -4.32\n",
      "Episode 1100\tLast reward: -8.00\tAverage reward: -5.16\n",
      "Episode 1200\tLast reward: -2.00\tAverage reward: -5.53\n",
      "Episode 1300\tLast reward: -13.00\tAverage reward: -5.51\n",
      "Episode 1400\tLast reward: -1.00\tAverage reward: -5.05\n",
      "Episode 1500\tLast reward: 0.00\tAverage reward: -3.65\n",
      "Episode 1600\tLast reward: 0.00\tAverage reward: -3.29\n",
      "Episode 1700\tLast reward: -20.00\tAverage reward: -5.16\n",
      "Episode 1800\tLast reward: -3.00\tAverage reward: -5.14\n",
      "Episode 1900\tLast reward: -4.00\tAverage reward: -4.36\n",
      "Episode 2000\tLast reward: -7.00\tAverage reward: -3.59\n",
      "Episode 2100\tLast reward: -10.00\tAverage reward: -5.39\n",
      "Episode 2200\tLast reward: 1.00\tAverage reward: -2.60\n",
      "Episode 2300\tLast reward: -11.00\tAverage reward: -3.74\n",
      "Episode 2400\tLast reward: 1.00\tAverage reward: -3.86\n",
      "Episode 2500\tLast reward: 0.00\tAverage reward: -3.93\n",
      "Episode 2600\tLast reward: -5.00\tAverage reward: -4.11\n",
      "Episode 2700\tLast reward: 0.00\tAverage reward: -5.27\n",
      "Episode 2800\tLast reward: -8.00\tAverage reward: -3.25\n",
      "Episode 2900\tLast reward: -7.00\tAverage reward: -3.59\n",
      "Episode 3000\tLast reward: -16.00\tAverage reward: -2.95\n",
      "Episode 3100\tLast reward: -3.00\tAverage reward: -4.69\n",
      "Episode 3200\tLast reward: -10.00\tAverage reward: -5.71\n",
      "Episode 3300\tLast reward: -2.00\tAverage reward: -2.95\n",
      "Episode 3400\tLast reward: -7.00\tAverage reward: -3.43\n",
      "Episode 3500\tLast reward: -2.00\tAverage reward: -2.20\n",
      "Episode 3600\tLast reward: 1.00\tAverage reward: -2.42\n",
      "Episode 3700\tLast reward: -7.00\tAverage reward: -3.79\n",
      "Episode 3800\tLast reward: -1.00\tAverage reward: -2.44\n",
      "Episode 3900\tLast reward: 0.00\tAverage reward: -2.47\n",
      "Episode 4000\tLast reward: -2.00\tAverage reward: -2.03\n",
      "Episode 4100\tLast reward: -11.00\tAverage reward: -3.13\n",
      "Episode 4200\tLast reward: -2.00\tAverage reward: -1.71\n",
      "Episode 4300\tLast reward: -11.00\tAverage reward: -2.72\n",
      "Episode 4400\tLast reward: -17.00\tAverage reward: -3.44\n",
      "Episode 4500\tLast reward: -1.00\tAverage reward: -1.90\n",
      "Episode 4600\tLast reward: 0.00\tAverage reward: -2.23\n",
      "Episode 4700\tLast reward: -4.00\tAverage reward: -1.39\n",
      "Episode 4800\tLast reward: 1.00\tAverage reward: -1.67\n",
      "Episode 4900\tLast reward: 0.00\tAverage reward: -2.73\n",
      "Episode 5000\tLast reward: -3.00\tAverage reward: -3.06\n",
      "Episode 5100\tLast reward: 1.00\tAverage reward: -1.75\n",
      "Episode 5200\tLast reward: -3.00\tAverage reward: -1.58\n",
      "Episode 5300\tLast reward: -5.00\tAverage reward: -2.71\n",
      "Episode 5400\tLast reward: -1.00\tAverage reward: -1.66\n",
      "Episode 5500\tLast reward: -1.00\tAverage reward: -1.08\n",
      "Episode 5600\tLast reward: -9.00\tAverage reward: -1.74\n",
      "Episode 5700\tLast reward: 1.00\tAverage reward: -1.31\n",
      "Episode 5800\tLast reward: 0.00\tAverage reward: -2.37\n",
      "Episode 5900\tLast reward: 0.00\tAverage reward: -1.23\n",
      "Episode 6000\tLast reward: -2.00\tAverage reward: -1.19\n",
      "Episode 6100\tLast reward: -3.00\tAverage reward: -1.78\n",
      "Episode 6200\tLast reward: 1.00\tAverage reward: -1.53\n",
      "Episode 6300\tLast reward: 0.00\tAverage reward: -1.34\n",
      "Episode 6400\tLast reward: -3.00\tAverage reward: -2.40\n",
      "Episode 6500\tLast reward: -1.00\tAverage reward: -0.90\n",
      "Episode 6600\tLast reward: 1.00\tAverage reward: -1.11\n",
      "Episode 6700\tLast reward: 0.00\tAverage reward: -0.81\n",
      "Episode 6800\tLast reward: 1.00\tAverage reward: -0.53\n",
      "Episode 6900\tLast reward: 1.00\tAverage reward: -1.04\n",
      "Episode 7000\tLast reward: 0.00\tAverage reward: -0.66\n",
      "Episode 7100\tLast reward: -1.00\tAverage reward: -1.25\n",
      "Episode 7200\tLast reward: -1.00\tAverage reward: -1.70\n",
      "Episode 7300\tLast reward: -9.00\tAverage reward: -0.73\n",
      "Episode 7400\tLast reward: -6.00\tAverage reward: -1.13\n",
      "Episode 7500\tLast reward: 1.00\tAverage reward: -0.90\n",
      "Episode 7600\tLast reward: -4.00\tAverage reward: -1.28\n",
      "Episode 7700\tLast reward: 0.00\tAverage reward: -1.29\n",
      "Episode 7800\tLast reward: 1.00\tAverage reward: -0.99\n",
      "Episode 7900\tLast reward: 1.00\tAverage reward: -1.59\n",
      "Episode 8000\tLast reward: 0.00\tAverage reward: -0.69\n",
      "Episode 8100\tLast reward: -3.00\tAverage reward: -0.38\n",
      "Episode 8200\tLast reward: 0.00\tAverage reward: -1.20\n",
      "Episode 8300\tLast reward: -1.00\tAverage reward: -1.38\n",
      "Episode 8400\tLast reward: 0.00\tAverage reward: -0.66\n",
      "Episode 8500\tLast reward: -1.00\tAverage reward: -0.97\n",
      "Episode 8600\tLast reward: -1.00\tAverage reward: -0.33\n",
      "Episode 8700\tLast reward: -1.00\tAverage reward: -0.61\n",
      "Episode 8800\tLast reward: 1.00\tAverage reward: -1.08\n",
      "Episode 8900\tLast reward: 0.00\tAverage reward: -0.90\n",
      "Episode 9000\tLast reward: 0.00\tAverage reward: -0.45\n",
      "Episode 9100\tLast reward: -5.00\tAverage reward: -0.70\n",
      "Episode 9200\tLast reward: -3.00\tAverage reward: -0.63\n",
      "Episode 9300\tLast reward: 1.00\tAverage reward: -0.04\n",
      "Episode 9400\tLast reward: 1.00\tAverage reward: 0.28\n",
      "Episode 9500\tLast reward: 1.00\tAverage reward: 0.25\n",
      "Episode 9600\tLast reward: 1.00\tAverage reward: -0.16\n",
      "Episode 9700\tLast reward: -1.00\tAverage reward: -0.50\n",
      "Episode 9800\tLast reward: 1.00\tAverage reward: -0.14\n",
      "Episode 9900\tLast reward: 1.00\tAverage reward: -0.41\n",
      "Episode 10000\tLast reward: 0.00\tAverage reward: -0.42\n",
      "Episode 10100\tLast reward: 1.00\tAverage reward: -0.06\n",
      "Episode 10200\tLast reward: 0.00\tAverage reward: -0.26\n",
      "Episode 10300\tLast reward: 1.00\tAverage reward: -0.45\n",
      "Episode 10400\tLast reward: 1.00\tAverage reward: 0.04\n",
      "Episode 10500\tLast reward: 1.00\tAverage reward: -0.65\n",
      "Episode 10600\tLast reward: 1.00\tAverage reward: -0.21\n",
      "Episode 10700\tLast reward: 1.00\tAverage reward: 0.19\n",
      "Episode 10800\tLast reward: 0.00\tAverage reward: -0.34\n",
      "Episode 10900\tLast reward: 1.00\tAverage reward: -0.09\n",
      "Episode 11000\tLast reward: -5.00\tAverage reward: -0.53\n",
      "Episode 11100\tLast reward: 0.00\tAverage reward: -0.62\n",
      "Episode 11200\tLast reward: 1.00\tAverage reward: 0.06\n",
      "Episode 11300\tLast reward: 1.00\tAverage reward: -0.70\n",
      "Episode 11400\tLast reward: -1.00\tAverage reward: -0.30\n",
      "Episode 11500\tLast reward: 1.00\tAverage reward: -0.07\n",
      "Episode 11600\tLast reward: 1.00\tAverage reward: -0.27\n",
      "Episode 11700\tLast reward: 0.00\tAverage reward: -0.35\n",
      "Episode 11800\tLast reward: 1.00\tAverage reward: 0.28\n",
      "Episode 11900\tLast reward: 1.00\tAverage reward: -0.31\n",
      "Episode 12000\tLast reward: -11.00\tAverage reward: -0.65\n",
      "Episode 12100\tLast reward: -9.00\tAverage reward: -0.79\n",
      "Episode 12200\tLast reward: 0.00\tAverage reward: 0.22\n",
      "Episode 12300\tLast reward: -3.00\tAverage reward: -0.30\n",
      "Episode 12400\tLast reward: 0.00\tAverage reward: 0.09\n",
      "Episode 12500\tLast reward: 1.00\tAverage reward: 0.26\n",
      "Episode 12600\tLast reward: -4.00\tAverage reward: -0.19\n",
      "Episode 12700\tLast reward: 0.00\tAverage reward: -0.26\n",
      "Episode 12800\tLast reward: 1.00\tAverage reward: -0.81\n",
      "Episode 12900\tLast reward: 0.00\tAverage reward: 0.11\n",
      "Episode 13000\tLast reward: 1.00\tAverage reward: 0.01\n",
      "Episode 13100\tLast reward: 1.00\tAverage reward: 0.04\n",
      "Episode 13200\tLast reward: -1.00\tAverage reward: -0.42\n",
      "Episode 13300\tLast reward: 1.00\tAverage reward: -0.27\n",
      "Episode 13400\tLast reward: 0.00\tAverage reward: -0.01\n",
      "Episode 13500\tLast reward: 1.00\tAverage reward: -0.18\n",
      "Episode 13600\tLast reward: 0.00\tAverage reward: -0.36\n",
      "Episode 13700\tLast reward: 0.00\tAverage reward: -0.37\n",
      "Episode 13800\tLast reward: 1.00\tAverage reward: -0.23\n",
      "Episode 13900\tLast reward: 1.00\tAverage reward: 0.41\n",
      "Episode 14000\tLast reward: -1.00\tAverage reward: -0.50\n",
      "Episode 14100\tLast reward: -4.00\tAverage reward: 0.05\n",
      "Episode 14200\tLast reward: -3.00\tAverage reward: -0.62\n",
      "Episode 14300\tLast reward: 1.00\tAverage reward: -0.00\n",
      "Episode 14400\tLast reward: -1.00\tAverage reward: -0.21\n",
      "Episode 14500\tLast reward: 1.00\tAverage reward: -0.04\n",
      "Episode 14600\tLast reward: 0.00\tAverage reward: -0.05\n",
      "Episode 14700\tLast reward: -2.00\tAverage reward: 0.03\n",
      "Episode 14800\tLast reward: -3.00\tAverage reward: -0.40\n",
      "Episode 14900\tLast reward: 0.00\tAverage reward: -0.07\n",
      "Episode 15000\tLast reward: 1.00\tAverage reward: -0.48\n",
      "Episode 15100\tLast reward: 0.00\tAverage reward: 0.02\n",
      "Episode 15200\tLast reward: -7.00\tAverage reward: 0.02\n",
      "Episode 15300\tLast reward: 1.00\tAverage reward: 0.15\n",
      "Episode 15400\tLast reward: 1.00\tAverage reward: -0.05\n",
      "Episode 15500\tLast reward: 1.00\tAverage reward: 0.26\n",
      "Episode 15600\tLast reward: -1.00\tAverage reward: -0.51\n",
      "Episode 15700\tLast reward: 1.00\tAverage reward: -0.32\n",
      "Episode 15800\tLast reward: 0.00\tAverage reward: -0.21\n",
      "Episode 15900\tLast reward: 0.00\tAverage reward: 0.06\n",
      "Episode 16000\tLast reward: 0.00\tAverage reward: 0.21\n",
      "Episode 16100\tLast reward: 0.00\tAverage reward: 0.08\n",
      "Episode 16200\tLast reward: 1.00\tAverage reward: 0.19\n",
      "Episode 16300\tLast reward: 1.00\tAverage reward: 0.16\n",
      "Episode 16400\tLast reward: -3.00\tAverage reward: 0.07\n",
      "Episode 16500\tLast reward: 1.00\tAverage reward: -0.13\n",
      "Episode 16600\tLast reward: 1.00\tAverage reward: -0.29\n",
      "Episode 16700\tLast reward: -1.00\tAverage reward: -0.02\n",
      "Episode 16800\tLast reward: 1.00\tAverage reward: 0.54\n",
      "Episode 16900\tLast reward: -4.00\tAverage reward: -0.24\n",
      "Episode 17000\tLast reward: -1.00\tAverage reward: 0.18\n",
      "Episode 17100\tLast reward: 1.00\tAverage reward: -0.00\n",
      "Episode 17200\tLast reward: 1.00\tAverage reward: 0.23\n",
      "Episode 17300\tLast reward: -2.00\tAverage reward: -0.29\n",
      "Episode 17400\tLast reward: -1.00\tAverage reward: -0.21\n",
      "Episode 17500\tLast reward: 0.00\tAverage reward: 0.03\n",
      "Episode 17600\tLast reward: -2.00\tAverage reward: -0.80\n",
      "Episode 17700\tLast reward: 1.00\tAverage reward: -0.12\n",
      "Episode 17800\tLast reward: 0.00\tAverage reward: -0.41\n",
      "Episode 17900\tLast reward: 0.00\tAverage reward: 0.11\n",
      "Episode 18000\tLast reward: 1.00\tAverage reward: -0.16\n",
      "Episode 18100\tLast reward: 1.00\tAverage reward: 0.14\n",
      "Episode 18200\tLast reward: 1.00\tAverage reward: 0.07\n",
      "Episode 18300\tLast reward: -1.00\tAverage reward: 0.05\n",
      "Episode 18400\tLast reward: 0.00\tAverage reward: 0.03\n",
      "Episode 18500\tLast reward: -2.00\tAverage reward: -0.17\n",
      "Episode 18600\tLast reward: 1.00\tAverage reward: 0.26\n",
      "Episode 18700\tLast reward: 0.00\tAverage reward: -0.71\n",
      "Episode 18800\tLast reward: 1.00\tAverage reward: 0.37\n",
      "Episode 18900\tLast reward: 1.00\tAverage reward: 0.29\n",
      "Episode 19000\tLast reward: 1.00\tAverage reward: 0.02\n",
      "Episode 19100\tLast reward: 1.00\tAverage reward: 0.22\n",
      "Episode 19200\tLast reward: -2.00\tAverage reward: -0.20\n",
      "Episode 19300\tLast reward: 1.00\tAverage reward: 0.15\n",
      "Episode 19400\tLast reward: 1.00\tAverage reward: -0.22\n",
      "Episode 19500\tLast reward: 0.00\tAverage reward: -0.00\n",
      "Episode 19600\tLast reward: -1.00\tAverage reward: -0.04\n",
      "Episode 19700\tLast reward: 0.00\tAverage reward: -0.36\n",
      "Episode 19800\tLast reward: 1.00\tAverage reward: -0.01\n",
      "Episode 19900\tLast reward: 1.00\tAverage reward: -0.54\n"
     ]
    }
   ],
   "source": [
    "agent = PGAgent(2, env.action_n, lr = LEARN_RATE, discounted_factor=DISCOUNTED_FACTOR)\n",
    "writer = SummaryWriter()\n",
    "num_episodes = 20000\n",
    "episode_len = 100\n",
    "epochs = 1\n",
    "iter_counter = 0\n",
    "# 第一次收集改为随机收集\n",
    "trajectory = []\n",
    "obs, _ = env.reset()\n",
    "# for _ in range(1000):\n",
    "#     state = tuple(obs['agent'])\n",
    "#     action = agent.get_behavior_action(state)\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     trajectory.append((state, action, reward+10))\n",
    "running_reward = -10\n",
    "for episode in range(num_episodes):\n",
    "    # 首先, 根据 policy 生成 episode\n",
    "    obs, _ = env.reset()\n",
    "    trajectory = []\n",
    "    ep_reward = 0\n",
    "    real_episode_len = 0\n",
    "    # 初始策略是不是有比较大的影响? \n",
    "    for real_episode_len in range(episode_len):\n",
    "        state = tuple(obs['agent'])\n",
    "        action = agent.get_action(state) # action 这里也有随机性\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        trajectory.append((state, action, reward))\n",
    "        ep_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    for epoch in range(epochs):\n",
    "        total_l = 0\n",
    "        discounted_reward = 0\n",
    "        for t, (state, action, reward) in enumerate(trajectory):\n",
    "        # for state, action, reward in reversed(trajectory):\n",
    "            # discounted_reward = sum(DISCOUNTED_FACTOR**i * t[2] for i, t in enumerate(trajectory[t:]))\n",
    "            discounted_reward = discounted_reward * agent.discounted_factor + reward\n",
    "            # policy update\n",
    "            \"\"\"\n",
    "            特别注意: 这里 log π 中的 π(a|s) 是选择 a 的概率, policy network 得输出一个概率, 而不是什么 a 的值\n",
    "            当然我们可以用输出的值, 归一化一下作为 action 的概率\n",
    "\n",
    "            有一个 变体可能, 在 sample action 时就计算 prob并存储, 然后在 update 时就只是计算 reward 从而计算 loss, 将一个 episode 的loss 都加到一起来一起 backward, 然后更新一次 policy network\n",
    "            感觉这种变体才是对的, action_prob , 现在计算的有点迟 (但是影响应该不大)\n",
    "            主要还是得加 advantage 的操作\n",
    "\n",
    "            \"\"\"\n",
    "            action_probs = agent.policy_net(torch.tensor(state, dtype=torch.float))\n",
    "            agent.q[state][action] = discounted_reward \n",
    "            agent.v[state] = sum([agent.q[state][a] * action_probs[a] for a in agent.q[state].keys()])\n",
    "            # agent.v[state] = np.mean(agent.q[state])\n",
    "\n",
    "            # action_probs = actions_val/actions_val.sum()\n",
    "            agent.optimizer.zero_grad()\n",
    "            \"\"\"\n",
    "            当 discounted reward < 0 时, loss < 0. 若是 action 选择错误, 则 discounted_reward 小, 使得 loss 小 (或者说负地厉害) \n",
    "            梯度下降会将 loss 减地更小, 也就使得对应错误 action 的 action_probs[action] 减小\n",
    "\n",
    "            相反, 当选择正确 action 时, discounted_reward 理想下应该更大, 则 loss 也更大, 梯度下降同样降低 loss,\n",
    "            使得对应正确 action 的 action_probs[action] 减小. \n",
    "            \n",
    "            关键就在于, 要使得 loss 小的时候梯度下降地比 loss 大的时候要更快.\n",
    "\n",
    "            (若是训练地成功 下一轮时 discounted_reward 就会变大, 那么 loss 也就是越来越大, 自然就是向上走,\n",
    "            至于为什么 loss 会趋近于 0, 我猜测是因为 discounted_reward 有一个由负变正的过程, 而在其中当 loss 变为 0 时\n",
    "            ) \n",
    "            \"\"\"\n",
    "            # loss = -torch.log(action_probs[action]) * (discounted_reward)\n",
    "            loss = -torch.log(action_probs[action]) * (discounted_reward - agent.v[state]) # add baselline advantage\n",
    "            # [parms.grad for name, parms in agent.policy_net.named_parameters()]\n",
    "            # loss = abs(loss)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad.clip_grad_norm_(agent.policy_net.parameters(), 100)\n",
    "            agent.optimizer.step()\n",
    "            writer.add_scalar('Loss', loss, iter_counter)\n",
    "            iter_counter+=1\n",
    "            total_l += loss\n",
    "        writer.add_scalar('episodeLoss', total_l, episode*epochs + epoch)\n",
    "        writer.add_scalar('episodeReward', discounted_reward, episode*epochs + epoch)\n",
    "        writer.add_scalar('ep_reward', ep_reward, episode*epochs + epoch)\n",
    "\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                episode, ep_reward, running_reward))\n",
    "    if running_reward > 0.6:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                \"the last episode runs to {} time steps!\".format(running_reward, real_episode_len))\n",
    "        break\n",
    "\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [[0.27010414 0.18948609 0.09536619 0.09522164 0.34982193]] [[0.46058697 0.08359051 0.06711915 0.06711915 0.32158417]] [[0.64286083 0.02568266 0.02568266 0.02568266 0.28009114]] ]\n",
      "[ [[0.09970145 0.48710915 0.06999383 0.06440948 0.27878618]] [[0.29346988 0.27921018 0.07374656 0.07013351 0.28343987]] [[0.7018638  0.0395993  0.02760158 0.02760158 0.20333369]] ]\n",
      "[ [[0.01471615 0.79551375 0.01542892 0.01471615 0.15962514]] [[0.02852456 0.77766603 0.01956342 0.01946699 0.15477894]] [[0.23398921 0.3908543  0.02886593 0.02886593 0.31742465]] ]\n",
      "[  ↺   ↓   ↓  ]\n",
      "[  →   ↓   ↓  ]\n",
      "[  →   →   →  ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "policy = agent.generate_policy_table(env.height, env.width)\n",
    "\n",
    "print_by_dict(env, policy)\n",
    "\n",
    "for i in range(env.height):\n",
    "    print(\"[\", end=\" \")\n",
    "    for j in range(env.width):\n",
    "        state = (i,j)\n",
    "        action = np.argmax(policy[state])\n",
    "        print(env.action_mappings[action], end=\" \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.max_steps = 10\n",
    "# gridworld_demo(agent, env, repeat_times=500)\n",
    "# gridworld_demo(agent, forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
