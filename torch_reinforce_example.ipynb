{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from rl_envs.gym_grid_world_env import GridWorldEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 1e-5\n",
    "DISCOUNTED_FACTOR = 0.9\n",
    "\n",
    "FORBIDDEN_REWARD = -1\n",
    "HITWALL_REWARD = -1\n",
    "TARGET_REWARD = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(2, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 5)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-8)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    discounted_reward = 0\n",
    "    policy_loss = []\n",
    "    inireturns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        discounted_reward = r + DISCOUNTED_FACTOR * discounted_reward\n",
    "        inireturns.insert(0, discounted_reward)\n",
    "    returns = torch.tensor(inireturns)\n",
    "    std = returns.std() if returns.size(0) != 1 else 0\n",
    "    returns = (returns - returns.mean()) / (std + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad.clip_grad_norm_(policy.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = -10\n",
    "    for i_episode in count(1):\n",
    "        ep_reward = 0\n",
    "        obs, _ = env.reset()\n",
    "        state = obs['agent']\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            obs, reward, terminate, truncated, _ = env.step(action)\n",
    "            state = obs['agent']\n",
    "            reward += 1\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if terminate or truncated:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * (ep_reward-t) + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 100 == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward-t, running_reward))\n",
    "        if running_reward > 0.8:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "def get_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    return torch.argmax(probs).item()\n",
    "\n",
    "def demo():\n",
    "    env = GridWorldEnv(render_mode=\"human\", size=3,fixed_map = True, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "    for i in range(env.size):\n",
    "        print(\"[\", end=\" \")\n",
    "        for j in range(env.size):\n",
    "            state = np.array([i, j])\n",
    "            # action = np.argmax(policy(state))\n",
    "            action = get_action(state)\n",
    "            print(env.action_mappings[action], end=\" \")\n",
    "        print(\"]\")\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    for i in range(500):\n",
    "        state = obs['agent']\n",
    "        action = get_action(state)\n",
    "        obs, reward, terminated, truncated, info  = env.step(action)\n",
    "        # VecEnv resets automatically\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            obs, _ = env.reset()\n",
    "            print('reward: {}, distance: {}'.format(total_reward, info))\n",
    "            total_reward = 0\n",
    "            if truncated:\n",
    "                print(\"TRUNCATE\")\n",
    "            else:\n",
    "                print(\"TERMINATE\")\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = GridWorldEnv(size=3,fixed_map = True, forbidden_grids=[(1,1)], target_grids=[(2,2)], forbidden_reward=FORBIDDEN_REWARD, hit_wall_reward=HITWALL_REWARD, target_reward=TARGET_REWARD)\n",
    "main()\n",
    "demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
