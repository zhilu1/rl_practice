{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rl_envs.grid_world_env import GridWorldEnv\n",
    "from agents.value_iteration_agent import ValueIterationAgent\n",
    "from agents.policy_iteration_agent import TruncatedPolicyIterationAgent\n",
    "# rl_envs.grid_world_env import GridWorldEnv\n",
    "\n",
    "%load_ext autoreload \n",
    "# %aimport rl_envs.grid_world_env\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: Opepossible_actionsI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.possible_actions is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.possible_actions containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.possible_actions)\n",
    "        for a in range(env.possible_actions):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.width * env.height)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.width * env.height):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.width * env.height, env.possible_actions])\n",
    "    for s in range(env.width * env.height):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = GridWorldEnv(5, 5, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=-1, hit_wall_reward=-1)\n",
    "env = GridWorldEnv(2, 2, forbidden_grids=[(0,1)], target_grids=[(1,1)])\n",
    "env.init_model_based_transitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.99915359 9.99915359 9.99915359 9.99915359]\n"
     ]
    }
   ],
   "source": [
    "policy, V = value_iteration(env, theta=0.0001, discount_factor=0.9)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration 部分完全一致\n",
    "\n",
    "我的结果为 {(0, 0): 8.999153585021714, (0, 1): 9.999153585021714, (1, 0): 9.999153585021714, (1, 1): 9.999153585021714})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  ↓   ↓  ]\n",
      "[  →   ↺  ]\n"
     ]
    }
   ],
   "source": [
    "def print_actions(policy, env):\n",
    "    index = 0\n",
    "    for i in range(env.height):\n",
    "        print(\"[\", end=\" \")\n",
    "        for j in range(env.width):\n",
    "            action = np.argmax(policy[index])\n",
    "            index+=1\n",
    "            print(env.action_mappings[action], end=\" \")\n",
    "        print(\"]\")\n",
    "\n",
    "print_actions(policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试更复杂的环境下的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.000000 0.000000 0.000000 0.000000 0.000000 ]\n",
      "[ 0.000000 -10.000000 -10.000000 0.000000 0.000000 ]\n",
      "[ 0.000000 0.000000 -10.000000 0.000000 0.000000 ]\n",
      "[ 0.000000 -10.000000 1.000000 -10.000000 0.000000 ]\n",
      "[ 0.000000 -10.000000 0.000000 0.000000 0.000000 ]\n",
      "\n",
      "[  →   →   →   →   ↓  ]\n",
      "[  ↑   ↑   →   →   ↓  ]\n",
      "[  ↑   ←   ↓   →   ↓  ]\n",
      "[  ↑   →   ↺   ←   ↓  ]\n",
      "[  ↑   →   ↑   ←   ←  ]\n",
      "[3.48616736 3.87358785 4.30405506 4.78235196 5.31379296 3.13755063\n",
      " 3.48622907 4.78235196 5.31379296 5.90428296 2.82379557 2.54141601\n",
      " 9.99915359 5.90428296 6.56038296 2.54141601 9.99915359 9.99915359\n",
      " 9.99923823 7.28938296 2.28727441 8.99923823 9.99923823 8.9993144\n",
      " 8.09938296]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorldEnv(5, 5, forbidden_grids=[(1,1),(1,2), (2,2),(3,1),(3,3),(4,1)], target_grids=[(3,2)], forbidden_reward=-10, hit_wall_reward=-1)\n",
    "# env = GridWorldEnv(2, 2, forbidden_grids=[(0,1)], target_grids=[(1,1)])\n",
    "env.init_model_based_transitions()\n",
    "\n",
    "policy, V = value_iteration(env, theta=0.0001, discount_factor=0.9)\n",
    "print(env)\n",
    "print_actions(policy, env)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_actions(agent, env):\n",
    "    index = 0\n",
    "    for i in range(env.height):\n",
    "        print(\"[\", end=\" \")\n",
    "        for j in range(env.width):\n",
    "            action = agent.get_action(index)\n",
    "            print(env.action_mappings[action], end=\" \")\n",
    "            index += 1\n",
    "        print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  →   →   →   →   ↓  ]\n",
      "[  ↑   ↑   →   →   ↓  ]\n",
      "[  ↑   ←   ↓   →   ↓  ]\n",
      "[  ↑   →   ↺   ←   ↓  ]\n",
      "[  ↑   →   ↑   ←   ←  ]\n",
      "dict_values([3.485937986021714, 3.8733584750217145, 4.303825685021715, 4.782122585021715, 5.313563585021715, 3.1372595459217134, 3.485937986021714, 4.782122585021715, 5.313563585021715, 5.904053585021714, 2.8234489498317146, 2.5410194133507136, 9.999153585021714, 5.904053585021714, 6.560153585021716, 2.5410194133507136, 9.999153585021714, 9.999153585021714, 9.999153585021714, 7.289153585021714, 2.286832830517813, 8.999153585021714, 9.999153585021714, 8.999153585021714, 8.099153585021716])\n"
     ]
    }
   ],
   "source": [
    "agent = ValueIterationAgent(action_space_n=env.possible_actions, discounted_factor=0.9, threshold=0.0001)\n",
    "agent.run(env)\n",
    "print_actions(agent, env)\n",
    "print(agent.v.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unmatch state value below:\n",
      "\n",
      "3.485937986021714 3.48616736448083 0.0002293784591160808\n",
      "3.8733584750217145 3.8735878534808297 0.0002293784591151926\n",
      "4.303825685021715 4.3040550634808294 0.00022937845911474852\n",
      "4.782122585021715 4.782351963480831 0.00022937845911652488\n",
      "5.313563585021715 5.31379296348083 0.00022937845911474852\n",
      "3.1372595459217134 3.137550628032747 0.00029108211103379134\n",
      "3.485937986021714 3.486229068132747 0.00029108211103290316\n",
      "4.782122585021715 4.782351963480831 0.00022937845911652488\n",
      "5.313563585021715 5.31379296348083 0.00022937845911474852\n",
      "5.904053585021714 5.904282963480831 0.00022937845911741306\n",
      "2.8234489498317146 2.8237955652294726 0.0003466153977580433\n",
      "2.5410194133507136 2.5414160087065256 0.0003965953558120461\n",
      "5.904053585021714 5.904282963480831 0.00022937845911741306\n",
      "6.560153585021716 6.56038296348083 0.00022937845911386034\n",
      "2.5410194133507136 2.5414160087065256 0.0003965953558120461\n",
      "9.999153585021714 9.999238226519543 8.464149782838604e-05\n",
      "7.289153585021714 7.289382963480829 0.00022937845911474852\n",
      "2.286832830517813 2.287274407835873 0.00044157731805993805\n",
      "8.999153585021714 8.999238226519543 8.464149782838604e-05\n",
      "9.999153585021714 9.999238226519543 8.464149782838604e-05\n",
      "8.999153585021714 8.999314403867588 0.00016081884587393347\n",
      "8.099153585021716 8.09938296348083 0.00022937845911386034\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnmatch state value below:\\n\")\n",
    "myV = list(agent.v.values())\n",
    "for i in range(len(V)):\n",
    "    if V[i] != myV[i]:\n",
    "        print(myV[i], V[i], V[i]-myV[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微小差异, 主要原因在于在更新 q 值时 使用的 V 值的差异, 按照 Shiyu Zhao 书中 V 值应该使用上一个 iteration 的 V 值来计算第 当前 iteration 的 Q 值的 future reward, 但是这份代码直接使用本次更新的 V 值 (更接近圣经的伪码)\n",
    "\n",
    "两种方法应该都能收敛到同一个地方, 区别不大"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
